{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Funcs.Utility import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_low_variance(agg_feature, threshold=.0000001):\n",
    "    agg_feature_non_zero_var = agg_feature.loc[:,agg_feature.var()>threshold]\n",
    "    num_removed = agg_feature.shape[1]-agg_feature_non_zero_var.shape[1]\n",
    "    print(f'{num_removed}/{agg_feature.shape[1]} features with variance < {threshold} removed')\n",
    "    return agg_feature_non_zero_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def remove_pairwise_corr(agg_feature_percent_missing, PAIRWISE_CORR_THRESHOLD=0.8, outcome_variable=None):\n",
    "    if outcome_variable is not None:\n",
    "        outcome_variable = pd.Series(outcome_variable, index=agg_feature_percent_missing.index, name=\"outcome\")\n",
    "        corr_with_outcome = pd.merge(outcome_variable, agg_feature_percent_missing, left_index=True, right_index=True).corr()[outcome_variable.name].abs().sort_values(ascending=False)\n",
    "        importance_order = corr_with_outcome.index[1:].tolist()\n",
    "        agg_feature_percent_missing = agg_feature_percent_missing[importance_order]\n",
    "\n",
    "    Matrix = agg_feature_percent_missing.corr().abs()\n",
    "    \n",
    "    upper_triangle = Matrix.where(np.triu(np.ones(Matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    correlated_features = set()\n",
    "    for feature in upper_triangle.columns:\n",
    "        highly_correlated = upper_triangle[feature][upper_triangle[feature] > PAIRWISE_CORR_THRESHOLD].index\n",
    "        correlated_features.update(highly_correlated)\n",
    "\n",
    "    kept_features = list(set(agg_feature_percent_missing.columns) - correlated_features)\n",
    "    print(f\"Pairwise Corr: kept only {len(kept_features)}/{len(agg_feature_percent_missing.columns)} features\")\n",
    "    return agg_feature_percent_missing[kept_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #LOGO + Timeseries k\n",
    "\n",
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "# from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# class CustomCV:\n",
    "#     def __init__(self, n_splits):\n",
    "#         self.n_splits = n_splits\n",
    "\n",
    "#     def split(self, X, y, groups):\n",
    "#         logo = LeaveOneGroupOut()\n",
    "\n",
    "#         for train_users, test_users in logo.split(X, y, groups):\n",
    "#             X_train_users, X_test_user = X.loc[train_users], X.loc[test_users]\n",
    "#             y_train_users, y_test_user = y[train_users], y[test_users]\n",
    "#             group_train_users, group_test_user = groups[train_users], groups[test_users]\n",
    "\n",
    "#             tscv = TimeSeriesSplit(n_splits=self.n_splits) \n",
    "\n",
    "#             # only take the first split\n",
    "#             train_index, test_index = next(tscv.split(X_test_user))\n",
    "            \n",
    "#             X_train, X_test = pd.concat([X_train_users, X_test_user.iloc[train_index]]), X_test_user.iloc[test_index]\n",
    "#             y_train, y_test = np.concatenate([y_train_users, y_test_user[train_index]]), y_test_user[test_index]\n",
    "\n",
    "#             yield (X_train.index, X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Time series partial personalization\n",
    "# #Logo + first 50% of data based on temporal order\n",
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "# from sklearn.model_selection import LeaveOneGroupOut\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# class CustomCV:\n",
    "#     def __init__(self, n_splits, test_ratio=0.9):\n",
    "#         self.n_splits = n_splits\n",
    "#         self.test_ratio = test_ratio  # Include a test_ratio parameter\n",
    "\n",
    "#     def split(self, X, y, groups):\n",
    "#         logo = LeaveOneGroupOut()\n",
    "\n",
    "#         for train_users, test_users in logo.split(X, y, groups):\n",
    "#             # This now splits the test users' data based on the specified ratio.\n",
    "#             X_train_users, X_test_user_full = X.loc[train_users], X.loc[test_users]\n",
    "#             y_train_users, y_test_user_full = y[train_users], y[test_users]\n",
    "#             groups_train_users, groups_test_user_full = groups[train_users], groups[test_users]\n",
    "            \n",
    "#             # Determine the split index for the test user's data\n",
    "#             split_index = int(len(X_test_user_full) * self.test_ratio)\n",
    "            \n",
    "#             # Split the test user's data into the part used for training and the actual test set\n",
    "#             X_test_user_train = X_test_user_full.iloc[:split_index]\n",
    "#             y_test_user_train = y_test_user_full[:split_index]\n",
    "#             X_test_user_test = X_test_user_full.iloc[split_index:]\n",
    "#             y_test_user_test = y_test_user_full[split_index:]\n",
    "            \n",
    "#             # Combine the other users' data with the part of the test user's data for training\n",
    "#             X_train_combined = pd.concat([X_train_users, X_test_user_train])\n",
    "#             y_train_combined = np.concatenate([y_train_users, y_test_user_train])\n",
    "\n",
    "#             # Yield the combined training set and the test set for the current fold\n",
    "#             yield (X_train_combined.index, X_test_user_test.index)\n",
    "\n",
    "# # ###Stratified partial personalization\n",
    "# from sklearn.model_selection import LeaveOneGroupOut, train_test_split\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# class CustomCV:\n",
    "#     def __init__(self, n_splits, test_size=0.5, random_state=42):\n",
    "#         self.test_size = test_size\n",
    "#         self.random_state = random_state\n",
    "\n",
    "#     def split(self, X, y, groups):\n",
    "#         logo = LeaveOneGroupOut()\n",
    "\n",
    "#         for train_idx, test_idx in logo.split(X, y, groups):\n",
    "#             # This now splits the test users' data based on the specified ratio.\n",
    "#             _, X_test_user_full = X.loc[train_idx], X.loc[test_idx]\n",
    "#             _, y_test_user_full = y[train_idx], y[test_idx]\n",
    "        \n",
    "            \n",
    "\n",
    "#             X_test_user_train, X_test_user_test, _, _ = train_test_split(\n",
    "#                     X_test_user_full, y_test_user_full, test_size=self.test_size, random_state=self.random_state, stratify=y_test_user_full\n",
    "#                 )\n",
    "\n",
    "            \n",
    "#             # Indices for the test group train set and the test set\n",
    "#             test_group_train_idx = X_test_user_train.index.values\n",
    "#             test_group_test_idx = X_test_user_test.index.values\n",
    "            \n",
    "#             # Combining the non-test group data indices and the test group train indices for training\n",
    "#             # This includes data from other users as well as 50% of the test user's data\n",
    "#             combined_train_idx = np.concatenate([train_idx, test_group_train_idx])\n",
    "            \n",
    "#             # Yielding the combined training indices and the test group test indices for this fold\n",
    "#             yield (combined_train_idx, test_group_test_idx)\n",
    "\n",
    "\n",
    "\n",
    "# # ###Random partial personalization\n",
    "# from sklearn.model_selection import LeaveOneGroupOut, train_test_split\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# class CustomCV:\n",
    "#     def __init__(self, n_splits, test_size=0.5, random_state=42):\n",
    "#         self.test_size = test_size\n",
    "#         self.random_state = random_state\n",
    "\n",
    "#     def split(self, X, y, groups):\n",
    "#         logo = LeaveOneGroupOut()\n",
    "\n",
    "#         for train_idx, test_idx in logo.split(X, y, groups):\n",
    "#             # This now splits the test users' data based on the specified ratio.\n",
    "#             X_train_users, X_test_user_full = X.loc[train_idx], X.loc[test_idx]\n",
    "#             y_train_users, y_test_user_full = y[train_idx], y[test_idx]\n",
    "        \n",
    "            \n",
    "\n",
    "#             X_test_user_train, X_test_user_test, _, _ = train_test_split(\n",
    "#                     X_test_user_full, y_test_user_full, test_size=self.test_size, random_state=self.random_state\n",
    "#                 )\n",
    "\n",
    "            \n",
    "#             # Indices for the test group train set and the test set\n",
    "#             test_group_train_idx = X_test_user_train.index.values\n",
    "#             test_group_test_idx = X_test_user_test.index.values\n",
    "            \n",
    "#             # Combining the non-test group data indices and the test group train indices for training\n",
    "#             # This includes data from other users as well as 50% of the test user's data\n",
    "#             combined_train_idx = np.concatenate([train_idx, test_group_train_idx])\n",
    "            \n",
    "#             # Yielding the combined training indices and the test group test indices for this fold\n",
    "#             yield (combined_train_idx, test_group_test_idx)\n",
    "\n",
    "\n",
    "\n",
    "# # ###LOSO (only using 50% for testing, rest unused, baseline 3)\n",
    "# from sklearn.model_selection import LeaveOneGroupOut, train_test_split\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# class CustomCV:\n",
    "#     def __init__(self, n_splits, test_size=0.5, random_state=42):\n",
    "#         self.test_size = test_size\n",
    "#         self.random_state = random_state\n",
    "\n",
    "#     def split(self, X, y, groups):\n",
    "#         logo = LeaveOneGroupOut()\n",
    "\n",
    "#         for train_idx, test_idx in logo.split(X, y, groups):\n",
    "#             # This now splits the test users' data based on the specified ratio.\n",
    "#             X_train_users, X_test_user_full = X.loc[train_idx], X.loc[test_idx]\n",
    "#             y_train_users, y_test_user_full = y[train_idx], y[test_idx]\n",
    "        \n",
    "            \n",
    "\n",
    "#             _, X_test_user_test, _, y_test_user_test = train_test_split(\n",
    "#                     X_test_user_full, y_test_user_full, test_size=self.test_size, random_state=self.random_state\n",
    "#                 )\n",
    "\n",
    "#             # Yield the combined training set and the test set for the current fold\n",
    "#             yield (train_idx, X_test_user_test.index.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "#Following is also for partial personalization. However, it is based on group k fold instead of LOSO.\n",
    "####################################\n",
    "\n",
    "\n",
    "\n",
    "# #Time series partial personalization\n",
    "# #Group k + first 50% of data based on temporal order\n",
    "# from sklearn.model_selection import  StratifiedGroupKFold\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# class CustomCV:\n",
    "#     def __init__(self, n_splits, test_ratio=0.9):\n",
    "#         self.n_splits = n_splits\n",
    "#         self.test_ratio = test_ratio  # Include a test_ratio parameter\n",
    "#         self.random_state = 42\n",
    "\n",
    "#     def split(self, X, y, groups):\n",
    "#         sgkf = StratifiedGroupKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
    "\n",
    "#         for train_users, test_users in sgkf.split(X, y, groups):\n",
    "#             # This now splits the test users' data based on the specified ratio.\n",
    "#             X_train_users, X_test_user_full = X.loc[train_users], X.loc[test_users]\n",
    "#             y_train_users, y_test_user_full = y[train_users], y[test_users]\n",
    "#             groups_train_users, groups_test_user_full = groups[train_users], groups[test_users]\n",
    "            \n",
    "#             # Determine the split index for the test user's data\n",
    "#             split_index = int(len(X_test_user_full) * self.test_ratio)\n",
    "            \n",
    "#             # Split the test user's data into the part used for training and the actual test set\n",
    "#             X_test_user_train = X_test_user_full.iloc[:split_index]\n",
    "#             y_test_user_train = y_test_user_full[:split_index]\n",
    "#             X_test_user_test = X_test_user_full.iloc[split_index:]\n",
    "#             y_test_user_test = y_test_user_full[split_index:]\n",
    "            \n",
    "#             # Combine the other users' data with the part of the test user's data for training\n",
    "#             X_train_combined = pd.concat([X_train_users, X_test_user_train])\n",
    "#             y_train_combined = np.concatenate([y_train_users, y_test_user_train])\n",
    "\n",
    "#             # Yield the combined training set and the test set for the current fold\n",
    "#             yield (X_train_combined.index, X_test_user_test.index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # #######Stratified partial personalization (group k version)\n",
    "# from sklearn.model_selection import train_test_split, StratifiedGroupKFold\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# class CustomCV:\n",
    "#     def __init__(self, n_splits=2, test_size=0.5, random_state=42):\n",
    "#         self.n_splits = n_splits\n",
    "#         self.test_size = test_size\n",
    "#         self.random_state = random_state\n",
    "\n",
    "#     def split(self, X, y, groups):\n",
    "#         sgkf = StratifiedGroupKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
    "\n",
    "#         for train_idx, test_idx in sgkf.split(X, y, groups=groups):\n",
    "#             # This now splits the test users' data based on the specified ratio.\n",
    "#             X_train_users, X_test_user_full = X.loc[train_idx], X.loc[test_idx]\n",
    "#             y_train_users, y_test_user_full = y[train_idx], y[test_idx]\n",
    "\n",
    "#             # Create a composite key for stratification (combine 'groups' and 'y' into a single stratify key)\n",
    "#             # We convert categories to strings and concatenate to ensure uniqueness\n",
    "#             stratify_key = ['{}_{}'.format(group, label) for group, label in zip(groups[test_idx], y_test_user_full)]\n",
    "\n",
    "#             # Split the test group's data to separate some data for training and the rest for testing\n",
    "#             X_test_group_train, X_test_group_test, y_test_group_train, y_test_group_test = train_test_split(\n",
    "#                 X_test_user_full, y_test_user_full, test_size=self.test_size,\n",
    "#                 stratify=stratify_key,  # Stratify by the composite key\n",
    "#                 random_state=self.random_state\n",
    "#             )\n",
    "\n",
    "#             # Convert DataFrame indices to numpy array indices (if using DataFrames)\n",
    "#             test_group_train_idx = X_test_group_train.index.values\n",
    "#             test_group_test_idx = X_test_group_test.index.values\n",
    "\n",
    "#             # Combine the indices of non-test group data with part of the test group's data for training\n",
    "#             combined_train_idx = np.concatenate([train_idx, test_group_train_idx])\n",
    "\n",
    "#             # Yield the indices for the training set and the test set for the current fold\n",
    "#             yield (combined_train_idx, test_group_test_idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # ###Random partial personalization (group k version)\n",
    "from sklearn.model_selection import train_test_split, StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class CustomCV:\n",
    "    def __init__(self, n_splits=2, test_size=0.5, random_state=42):\n",
    "        self.n_splits = n_splits\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def split(self, X, y, groups):\n",
    "        sgkf = StratifiedGroupKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
    "\n",
    "        # Use StratifiedGroupKFold to maintain percentage of samples for each class\n",
    "        for train_idx, test_idx in sgkf.split(X, y, groups=groups):\n",
    "            # This now splits the test users' data based on the specified ratio.\n",
    "            X_train_users, X_test_user_full = X.loc[train_idx], X.loc[test_idx]\n",
    "            y_train_users, y_test_user_full = y[train_idx], y[test_idx]\n",
    "            \n",
    "\n",
    "            # Split the test group's data into a training set and a test set\n",
    "            X_test_group_train, X_test_group_test, y_test_group_train, y_test_group_test = train_test_split(\n",
    "                    X_test_user_full, y_test_user_full, test_size=self.test_size, \n",
    "                    random_state=self.random_state\n",
    "                )\n",
    "\n",
    "            # Extract indices for combined training set and test set\n",
    "            test_group_train_idx = X_test_group_train.index.values\n",
    "            test_group_test_idx = X_test_group_test.index.values\n",
    "            \n",
    "            # Combine other users' data (from training fold) with the part-training set of the test user\n",
    "            combined_train_idx = np.concatenate([train_idx, test_group_train_idx])\n",
    "            \n",
    "            # Yield the indices for combined training and the test set for current fold\n",
    "            yield (combined_train_idx, test_group_test_idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # # ###Group k (only random 50% for testing and rest not used for training)\n",
    "# from sklearn.model_selection import train_test_split, StratifiedGroupKFold\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# class CustomCV:\n",
    "#     def __init__(self, n_splits=2, test_size=0.5, random_state=42):\n",
    "#         self.n_splits = n_splits\n",
    "#         self.test_size = test_size\n",
    "#         self.random_state = random_state\n",
    "\n",
    "#     def split(self, X, y, groups):\n",
    "#         sgkf = StratifiedGroupKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
    "\n",
    "#         # Use StratifiedGroupKFold to maintain percentage of samples for each class\n",
    "#         for train_idx, test_idx in sgkf.split(X, y, groups=groups):\n",
    "#             # This now splits the test users' data based on the specified ratio.\n",
    "#             X_train_users, X_test_user_full = X.loc[train_idx], X.loc[test_idx]\n",
    "#             y_train_users, y_test_user_full = y[train_idx], y[test_idx]\n",
    "            \n",
    "\n",
    "#             # Split the test group's data into a training set and a test set\n",
    "#             X_test_group_train, X_test_group_test, y_test_group_train, y_test_group_test = train_test_split(\n",
    "#                     X_test_user_full, y_test_user_full, test_size=self.test_size, \n",
    "#                     random_state=self.random_state\n",
    "#                 )\n",
    "\n",
    "#             # Extract indices for combined training set and test set\n",
    "#             test_group_train_idx = X_test_group_train.index.values\n",
    "#             test_group_test_idx = X_test_group_test.index.values\n",
    "            \n",
    "#             # Combine other users' data (from training fold) with the part-training set of the test user\n",
    "#             combined_train_idx = np.concatenate([train_idx, test_group_train_idx])\n",
    "            \n",
    "#             # Yield the indices for combined training and the test set for current fold\n",
    "#             yield (train_idx, test_group_test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback as tb\n",
    "from contextlib import contextmanager\n",
    "from typing import Tuple, Dict, Union, Generator, List\n",
    "from dataclasses import dataclass\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, LeaveOneGroupOut, StratifiedShuffleSplit, RepeatedStratifiedKFold, StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "import time\n",
    "import ray\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM\n",
    "# from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FoldResult:\n",
    "    name: str\n",
    "    estimator: BaseEstimator\n",
    "    X_train: pd.DataFrame\n",
    "    y_train: np.ndarray\n",
    "    X_test: pd.DataFrame\n",
    "    y_test: np.ndarray\n",
    "    categories: Dict[str, Dict[int, str]] = None\n",
    "    datetimes_train: np.ndarray = None\n",
    "    datetimes_test: np.ndarray = None\n",
    "\n",
    "\n",
    "\n",
    "def _split(\n",
    "        alg: str,\n",
    "        X: Union[pd.DataFrame, np.ndarray] = None,\n",
    "        y: np.ndarray = None,\n",
    "        groups: np.ndarray = None,\n",
    "        random_state: int = None,\n",
    "        n_splits: int = None,\n",
    "        n_repeats: int = None,\n",
    "        test_ratio: float = None\n",
    ") -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
    "    if alg == 'holdout':\n",
    "        splitter = StratifiedShuffleSplit(\n",
    "            n_splits=n_splits,\n",
    "            test_size=test_ratio,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    elif alg == 'kfold':\n",
    "        if n_repeats and n_repeats > 1:\n",
    "            splitter = RepeatedStratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                n_repeats=n_repeats,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        else:\n",
    "            splitter = StratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                random_state=random_state,\n",
    "                shuffle=False if random_state is None else True,\n",
    "            )\n",
    "    elif alg == 'logo':\n",
    "        splitter = LeaveOneGroupOut()\n",
    "    elif alg == 'groupk':\n",
    "        splitter = StratifiedGroupKFold(n_splits=n_splits)\n",
    "    elif alg == 'TimeSeriesSplit':\n",
    "        splitter = TimeSeriesSplit(n_splits=n_splits)\n",
    "    elif alg == 'custom_cv':\n",
    "        splitter = CustomCV(n_splits=n_splits)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('\"alg\" should be one of \"holdout\", \"kfold\", \"logo\", \"TimeSeriesSplit\", \"custom_cv\" or \"groupk\".')\n",
    "\n",
    "    split = splitter.split(X, y, groups)\n",
    "\n",
    "    for I_train, I_test in split:\n",
    "        yield I_train, I_test\n",
    "\n",
    "\n",
    "def _train(\n",
    "    dir_result: str,\n",
    "    name: str,\n",
    "    datetimes_train: np.ndarray,  # Add datetimes_train parameter\n",
    "    datetimes_test: np.ndarray,  # Add datetimes_test parameter\n",
    "\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: np.ndarray,\n",
    "    C_cat: np.ndarray,\n",
    "    C_num: np.ndarray,\n",
    "    estimator: BaseEstimator,\n",
    "    normalize: bool = False,\n",
    "    select: Union[List[SelectFromModel], SelectFromModel] = None,\n",
    "    oversample: bool = False,\n",
    "    random_state: int = None,\n",
    "    categories: Union[List, Dict[str, Dict[int, str]]] = None\n",
    "\n",
    "\n",
    "):\n",
    "    @contextmanager\n",
    "    def _log(task_type: str):\n",
    "        log(f'In progress: {task_type}.')\n",
    "        _t = time.time()\n",
    "        _err = None\n",
    "        _result = dict()\n",
    "        \n",
    "        try:\n",
    "            yield _result\n",
    "        except:\n",
    "            _err = tb.format_exc()\n",
    "        finally:\n",
    "            _e = time.time() - _t\n",
    "            if _err:\n",
    "                _msg = f'Failure: {task_type} ({_e:.2f}s). Keep running without this task. Caused by: \\n{_err}' \n",
    "            else:\n",
    "                _msg = f'Success: {task_type} ({_e:.2f}s).' \n",
    "                if _result:\n",
    "                    _r = '\\n'.join([f'- {k}: {v}' for k, v in _result.items()])\n",
    "                    _msg = f'{_msg}\\n{_r}'\n",
    "            log(_msg)\n",
    "#         #Instead of using fixed threshold, we tried to use mean value as threshold for binarization\n",
    "# #     y_train_mean = np.mean(np.concatenate((y_train,y_test)))\n",
    "#     y_train_mean = np.mean(y_train)\n",
    "#     y_train = np.where(y_train > y_train_mean, 1, 0)\n",
    "#     y_test= np.where(y_test > y_train_mean, 1, 0)\n",
    "# #     X_train['ESM#LastLabel'] = np.where(X_train['ESM#LastLabel'] > y_train_mean, 1, 0)\n",
    "# #     X_test['ESM#LastLabel'] = np.where(X_test['ESM#LastLabel'] > y_train_mean, 1, 0)\n",
    "    \n",
    "    if normalize:\n",
    "        with _log(f'[{name}] Normalizing numeric features'):\n",
    "            X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "            X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "            \n",
    "            scaler = StandardScaler().fit(X_train_N)\n",
    "            X_train_N = scaler.transform(X_train_N)\n",
    "            X_test_N = scaler.transform(X_test_N)\n",
    "         \n",
    "            X_train = pd.DataFrame(\n",
    "                np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "            X_test = pd.DataFrame(\n",
    "                np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "           \n",
    "    if select:\n",
    "        # # Removing low variance features\n",
    "        # X_train = exclude_low_variance(X_train)\n",
    "        # X_test = X_test[X_train.columns]  # Keep only the selected features in the test set\n",
    "\n",
    "        # #Removing highly correlated features\n",
    "        # X_train = remove_pairwise_corr(X_train, outcome_variable= y_train)\n",
    "        # X_test = X_test[X_train.columns]  # Keep only the selected features in the test set\n",
    "\n",
    "        if isinstance(select, SelectFromModel):\n",
    "            select = [select]\n",
    "            \n",
    "        for i, s in enumerate(select):\n",
    "            with _log(f'[{name}] {i+1}-th Feature selection') as r:\n",
    "                C = np.asarray(X_train.columns)\n",
    "                r['# Orig. Feat.'] = f'{len(C)} (# Cat. = {len(C_cat)}; # Num. = {len(C_num)})'\n",
    "                M = s.fit(X=X_train.values, y=y_train).get_support()\n",
    "                C_sel = C[M]\n",
    "                C_cat = C_cat[np.isin(C_cat, C_sel)]\n",
    "                C_num = C_num[np.isin(C_num, C_sel)]\n",
    "                \n",
    "                X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "                X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "\n",
    "\n",
    "                X_train = pd.DataFrame(\n",
    "                    np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                X_test = pd.DataFrame(\n",
    "                    np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                r['# Sel. Feat.'] = f'{len(C_sel)} (# Cat. = {len(C_cat)}; # Num. = {len(C_num)})'\n",
    "\n",
    "    if oversample:\n",
    "        with _log(f'[{name}] Oversampling') as r:\n",
    "            if len(C_cat):\n",
    "                M = np.isin(X_train.columns, C_cat)\n",
    "                sampler = SMOTENC(categorical_features=M, random_state=random_state)\n",
    "                # sampler = RandomOverSampler(random_state=random_state)\n",
    "                # sampler = RandomUnderSampler(random_state=random_state)\n",
    "            else:\n",
    "                sampler = SMOTE(random_state=random_state)\n",
    "                # sampler = RandomOverSampler(random_state=random_state)\n",
    "                # sampler = RandomUnderSampler(random_state=random_state)\n",
    "\n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "#             # Create oversampled datetimes_train\n",
    "#             datetimes_train_oversampled = np.repeat(datetimes_train, sampler.sample_indices_.shape[0], axis=0)\n",
    "\n",
    "    # You can access the underlying model class like this:\n",
    "\n",
    "    with _log(f'[{name}] Training'):\n",
    "        estimator = estimator.fit(X_train, y_train)\n",
    "        result = FoldResult(\n",
    "            name=name,\n",
    "            estimator=estimator,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            categories=categories\n",
    "        )\n",
    "        dump(result, os.path.join(dir_result, f'{name}.pkl'))\n",
    "    \n",
    "\n",
    "def cross_val(\n",
    "    X: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    groups: np.ndarray,\n",
    "    datetimes:  np.ndarray,\n",
    "    path: str,\n",
    "    name: str,\n",
    "    estimator: BaseEstimator,\n",
    "    categories: List[str] = None,\n",
    "    normalize: bool = False,\n",
    "    split: str = None,\n",
    "    split_params: Dict[str, any] = None,\n",
    "    select: Union[List[SelectFromModel], SelectFromModel] = None,\n",
    "    oversample: bool = False,\n",
    "    random_state: int = None\n",
    "\n",
    "):\n",
    "    if not os.path.exists(path):\n",
    "        raise ValueError('\"path\" does not exist.')\n",
    "    \n",
    "    if not split:\n",
    "        raise ValueError('\"split\" should be specified.')\n",
    "    \n",
    "    if not ray.is_initialized():\n",
    "        raise EnvironmentError('\"ray\" should be initialized.')\n",
    "    \n",
    "    jobs = []\n",
    "    func = ray.remote(_train).remote\n",
    "\n",
    "    categories = list() if categories is None else categories\n",
    "    C_cat = np.asarray(sorted(categories))\n",
    "    C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))\n",
    "\n",
    "    split_params = split_params or dict()\n",
    "    splitter = _split(alg=split, X=X, y=y, groups=groups, random_state=random_state, **split_params)\n",
    "    \n",
    "    \n",
    "\n",
    "    for idx_fold, (I_train, I_test) in enumerate(splitter):\n",
    "        if split == 'logo':\n",
    "            FOLD_NAME = str(np.unique(groups[I_test]).item(0))\n",
    "        else:\n",
    "            FOLD_NAME = str(idx_fold + 1)\n",
    "\n",
    "        X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "        X_test, y_test = X.iloc[I_test, :], y[I_test]\n",
    "        datetimes_train, datetimes_test = datetimes[I_train], datetimes[I_test]  # Add datetimes_train and datetimes_test\n",
    "\n",
    "\n",
    "        job = func(\n",
    "            dir_result=path,\n",
    "            \n",
    "            datetimes_train=datetimes_train,  # Pass datetimes_train\n",
    "            datetimes_test=datetimes_test,  # Pass datetimes_test\n",
    "\n",
    "            name=f'{name}#{FOLD_NAME}',\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            C_cat=C_cat,\n",
    "            C_num=C_num,\n",
    "            categories=categories,\n",
    "            estimator=clone(estimator),\n",
    "            normalize=normalize,\n",
    "            select=select,\n",
    "            oversample=oversample,\n",
    "            random_state=random_state\n",
    "\n",
    "        )\n",
    "        jobs.append(job)\n",
    "    ray.get(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minor modification on XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This modification allows XGBClassifiers to automatically generate evaluation sets during pipeline (without passing any argument in \"fit\" function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class EvXGBClassifier(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_size=None,\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=10,\n",
    "        random_state=None,\n",
    "        **kwargs\n",
    "        ):\n",
    "        self.random_state = random_state\n",
    "        self.eval_size = eval_size\n",
    "        self.eval_metric = eval_metric\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.model = XGBClassifier(\n",
    "            random_state=self.random_state,\n",
    "            eval_metric=self.eval_metric,\n",
    "            early_stopping_rounds=self.early_stopping_rounds,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def classes_(self):\n",
    "        return self.model.classes_\n",
    "\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        return self.model.feature_importances_\n",
    "    \n",
    "    @property\n",
    "    def feature_names_in_(self):\n",
    "        return self.model.feature_names_in_\n",
    "\n",
    "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: np.ndarray):\n",
    "        if self.eval_size:\n",
    "            splitter = StratifiedShuffleSplit(random_state=self.random_state, test_size=self.eval_size)\n",
    "            I_train, I_eval = next(splitter.split(X, y))\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X.iloc[I_eval, :], y[I_eval]\n",
    "            else:\n",
    "                X_train, y_train = X[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X[I_eval, :], y[I_eval]\n",
    "                \n",
    "            self.model = self.model.fit(\n",
    "                X=X_train, y=y_train, \n",
    "                eval_set=[(X_eval, y_eval)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            self.model = self.model.fit(X=X, y=y, verbose=False)\n",
    "        # After fitting, store the best iteration\n",
    "        self.best_iteration_ = self.model.get_booster().best_iteration\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "#         return self.model.predict(X)\n",
    "        return self.model.predict(X, iteration_range=(0, self.best_iteration_ + 1))\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame):\n",
    "#         return self.model.predict_proba(X)\n",
    "        return self.model.predict_proba(X, iteration_range=(0, self.best_iteration_ + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_PROC = pd.read_csv(os.path.join(PATH_INTERMEDIATE, 'Preprocessed', 'LABELS_PROC.csv'), index_col=['uid','timestamp'],parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following chunk is for user specific threshold\n",
    "import numpy as np\n",
    "\n",
    "def zscore(col):\n",
    "  mean = col.mean()\n",
    "  std = col.std()\n",
    "  return (col - mean) / std\n",
    "\n",
    "#Use user speicifc mean threshold\n",
    "\n",
    "LABELS_PROC['stress_user_mean'] = np.nan\n",
    "\n",
    "for user in LABELS_PROC.index.get_level_values('uid').unique():\n",
    "    user_df = LABELS_PROC.loc[user].copy()\n",
    "    user_df['zscore'] = zscore(user_df['stressLevel']) \n",
    "\n",
    "    # Align indices before assigning\n",
    "    user_df = user_df.align(LABELS_PROC, axis=0)[0]  \n",
    "\n",
    "    LABELS_PROC.loc[user, 'stress_user_mean'] = (user_df['zscore'] > user_df['zscore'].mean()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #The following code is designed for excluding the 1st day's data\n",
    "# ##############################################\n",
    "# _df =LABELS_PROC\n",
    "# _df.reset_index(level='timestamp', inplace=True)\n",
    "# print('First timestamp:', _df['timestamp'].min())\n",
    "# print('Last timestamp:', _df['timestamp'].max())\n",
    "# time_ranges = _df.groupby('uid')['timestamp'].agg(['min', 'max'])\n",
    "# print(time_ranges)\n",
    "# def exclude_first_day(df):\n",
    "#     # Get the date of the first timestamp\n",
    "#     first_date = df['timestamp'].iloc[0].date()\n",
    "    \n",
    "#     # Get the date of the second day\n",
    "#     second_date = first_date + pd.Timedelta(days=1)\n",
    "    \n",
    "#     # Only keep rows where the date is on or after the second day\n",
    "#     df = df[df['timestamp'].dt.date >= second_date]\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# df = LABELS_PROC.reset_index()\n",
    "\n",
    "# # Apply the function to each group\n",
    "# filtered_df = df.groupby('uid', group_keys=False).apply(exclude_first_day)\n",
    "# filtered_df.to_csv(os.path.join(PATH_INTERMEDIATE,'exclude_1st_day.csv'),index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # Assuming LABELS_PROC is a DataFrame with a 'timestamp' index and a 'uid' column\n",
    "# _df = LABELS_PROC\n",
    "# _df.reset_index(level='timestamp', inplace=True)\n",
    "# print('First timestamp:', _df['timestamp'].min())\n",
    "# print('Last timestamp:', _df['timestamp'].max())\n",
    "\n",
    "# # Calculate the min and max timestamps for each user\n",
    "# time_ranges = _df.groupby('uid')['timestamp'].agg(['min', 'max'])\n",
    "# print(time_ranges)\n",
    "\n",
    "# def exclude_first_week(df):\n",
    "#     # Get the datetime of the first timestamp\n",
    "#     first_datetime = df['timestamp'].iloc[0]\n",
    "    \n",
    "#     # Calculate the datetime exactly one week after the first timestamp\n",
    "#     one_week_later = first_datetime + pd.Timedelta(weeks=1)\n",
    "    \n",
    "#     # Filter out rows where the datetime is less than one week after the first recorded datetime\n",
    "#     df = df[df['timestamp'] >= one_week_later]\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # Reset index if your 'timestamp' is not already an index, ensuring 'uid' is included for grouping\n",
    "# df = LABELS_PROC.reset_index()\n",
    "\n",
    "# # Apply the function to exclude data from the first week for each user/group\n",
    "# filtered_df = df.groupby('uid', group_keys=False).apply(exclude_first_week)\n",
    "\n",
    "# # Optionally, save the filtered data to a CSV file\n",
    "# filtered_df.to_csv(os.path.join(PATH_INTERMEDIATE, 'exclude_1st_week.csv'), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 13:23:37,181\tINFO worker.py:1612 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_train pid=1132300)\u001b[0m [24-04-24 13:23:39] In progress: [xgb_os#P03] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=1132306)\u001b[0m [24-04-24 13:23:39] Success: [xgb_os#P04] Normalizing numeric features (0.05s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132306)\u001b[0m [24-04-24 13:23:39] In progress: [xgb_os#P04] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=1132314)\u001b[0m [24-04-24 13:24:02] Success: [xgb_os#P02] 1-th Feature selection (22.47s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132314)\u001b[0m - # Orig. Feat.: 191 (# Cat. = 26; # Num. = 165)\n",
      "\u001b[2m\u001b[36m(_train pid=1132314)\u001b[0m - # Sel. Feat.: 158 (# Cat. = 19; # Num. = 139)\n",
      "\u001b[2m\u001b[36m(_train pid=1132314)\u001b[0m [24-04-24 13:24:02] In progress: [xgb_os#P02] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=1132301)\u001b[0m [24-04-24 13:23:41] In progress: [xgb_os#P19] Normalizing numeric features.\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132301)\u001b[0m [24-04-24 13:23:42] Success: [xgb_os#P19] Normalizing numeric features (0.11s).\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132301)\u001b[0m [24-04-24 13:23:42] In progress: [xgb_os#P19] 1-th Feature selection.\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132310)\u001b[0m [24-04-24 13:24:09] Success: [xgb_os#P14] 1-th Feature selection (27.60s).\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132310)\u001b[0m - # Orig. Feat.: 191 (# Cat. = 26; # Num. = 165)\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132310)\u001b[0m - # Sel. Feat.: 163 (# Cat. = 19; # Num. = 144)\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132310)\u001b[0m [24-04-24 13:24:09] In progress: [xgb_os#P14] Oversampling.\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132303)\u001b[0m [24-04-24 13:26:17] Success: [xgb_os#P05] Oversampling (132.99s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132303)\u001b[0m [24-04-24 13:26:17] In progress: [xgb_os#P05] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=1132314)\u001b[0m [24-04-24 13:26:23] Success: [xgb_os#P02] Oversampling (140.91s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132314)\u001b[0m [24-04-24 13:26:23] In progress: [xgb_os#P02] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=1132309)\u001b[0m [24-04-24 13:26:27] Success: [xgb_os#P15] Oversampling (141.13s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132309)\u001b[0m [24-04-24 13:26:27] In progress: [xgb_os#P15] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=1132305)\u001b[0m [24-04-24 13:26:33] Success: [xgb_os#P17] Oversampling (147.80s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132305)\u001b[0m [24-04-24 13:26:33] In progress: [xgb_os#P17] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=1132304)\u001b[0m [24-04-24 13:26:36] Success: [xgb_os#P08] Oversampling (152.32s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132304)\u001b[0m [24-04-24 13:26:36] In progress: [xgb_os#P08] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=1132312)\u001b[0m [24-04-24 13:26:45] Success: [xgb_os#P07] Oversampling (160.01s).\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132312)\u001b[0m [24-04-24 13:26:45] In progress: [xgb_os#P07] Training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132303)\u001b[0m [24-04-24 13:26:49] Success: [xgb_os#P05] Training (32.80s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132303)\u001b[0m [24-04-24 13:26:49] In progress: [xgb_os#P20] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=1132303)\u001b[0m [24-04-24 13:26:49] Success: [xgb_os#P20] Normalizing numeric features (0.02s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132303)\u001b[0m [24-04-24 13:26:49] In progress: [xgb_os#P20] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=1132299)\u001b[0m [24-04-24 13:26:53] Success: [xgb_os#P11] Oversampling (167.12s).\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132299)\u001b[0m [24-04-24 13:26:53] In progress: [xgb_os#P11] Training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132314)\u001b[0m [24-04-24 13:26:54] Success: [xgb_os#P02] Training (30.30s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132314)\u001b[0m [24-04-24 13:26:54] In progress: [xgb_os#P21] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=1132314)\u001b[0m [24-04-24 13:26:54] Success: [xgb_os#P21] Normalizing numeric features (0.03s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132314)\u001b[0m [24-04-24 13:26:54] In progress: [xgb_os#P21] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=1132313)\u001b[0m [24-04-24 13:26:58] Success: [xgb_os#P12] Oversampling (174.19s).\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132313)\u001b[0m [24-04-24 13:26:58] In progress: [xgb_os#P12] Training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132309)\u001b[0m [24-04-24 13:26:58] Success: [xgb_os#P15] Training (31.60s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132309)\u001b[0m [24-04-24 13:26:59] In progress: [xgb_os#P22] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=1132309)\u001b[0m [24-04-24 13:26:59] Success: [xgb_os#P22] Normalizing numeric features (0.04s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132309)\u001b[0m [24-04-24 13:26:59] In progress: [xgb_os#P22] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=1132314)\u001b[0m [24-04-24 13:27:02] Success: [xgb_os#P21] 1-th Feature selection (8.01s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132314)\u001b[0m - # Orig. Feat.: 191 (# Cat. = 26; # Num. = 165)\n",
      "\u001b[2m\u001b[36m(_train pid=1132314)\u001b[0m - # Sel. Feat.: 159 (# Cat. = 18; # Num. = 141)\n",
      "\u001b[2m\u001b[36m(_train pid=1132314)\u001b[0m [24-04-24 13:27:02] In progress: [xgb_os#P21] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=1132310)\u001b[0m [24-04-24 13:26:58] Success: [xgb_os#P14] Oversampling (169.67s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132310)\u001b[0m [24-04-24 13:26:58] In progress: [xgb_os#P14] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=1132305)\u001b[0m [24-04-24 13:27:06] Success: [xgb_os#P17] Training (32.61s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132305)\u001b[0m [24-04-24 13:27:06] In progress: [xgb_os#P23] Normalizing numeric features.\n",
      "\u001b[2m\u001b[36m(_train pid=1132305)\u001b[0m [24-04-24 13:27:06] Success: [xgb_os#P23] Normalizing numeric features (0.06s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132305)\u001b[0m [24-04-24 13:27:06] In progress: [xgb_os#P23] 1-th Feature selection.\n",
      "\u001b[2m\u001b[36m(_train pid=1132304)\u001b[0m [24-04-24 13:27:06] Success: [xgb_os#P08] Training (30.35s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132309)\u001b[0m [24-04-24 13:27:07] Success: [xgb_os#P22] 1-th Feature selection (8.60s).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132309)\u001b[0m - # Orig. Feat.: 191 (# Cat. = 26; # Num. = 165)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132309)\u001b[0m - # Sel. Feat.: 163 (# Cat. = 18; # Num. = 145)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132309)\u001b[0m [24-04-24 13:27:07] In progress: [xgb_os#P22] Oversampling.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132308)\u001b[0m [24-04-24 13:27:09] Success: [xgb_os#P18] Oversampling (182.31s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132308)\u001b[0m [24-04-24 13:27:09] In progress: [xgb_os#P18] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=1132311)\u001b[0m [24-04-24 13:27:10] In progress: [xgb_os#P25] Normalizing numeric features.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132311)\u001b[0m [24-04-24 13:27:11] Success: [xgb_os#P25] Normalizing numeric features (0.04s).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132311)\u001b[0m [24-04-24 13:27:11] In progress: [xgb_os#P25] 1-th Feature selection.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132300)\u001b[0m [24-04-24 13:27:15] Success: [xgb_os#P03] Training (37.71s).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132304)\u001b[0m [24-04-24 13:27:15] Success: [xgb_os#P24] 1-th Feature selection (9.04s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132304)\u001b[0m - # Orig. Feat.: 191 (# Cat. = 26; # Num. = 165)\n",
      "\u001b[2m\u001b[36m(_train pid=1132304)\u001b[0m - # Sel. Feat.: 163 (# Cat. = 19; # Num. = 144)\n",
      "\u001b[2m\u001b[36m(_train pid=1132304)\u001b[0m [24-04-24 13:27:15] In progress: [xgb_os#P24] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=1132302)\u001b[0m [24-04-24 13:27:19] In progress: [xgb_os#P27] Normalizing numeric features.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132302)\u001b[0m [24-04-24 13:27:19] Success: [xgb_os#P27] Normalizing numeric features (0.03s).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132302)\u001b[0m [24-04-24 13:27:19] In progress: [xgb_os#P27] 1-th Feature selection.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132312)\u001b[0m [24-04-24 13:27:19] Success: [xgb_os#P07] Training (33.42s).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132311)\u001b[0m [24-04-24 13:27:20] Success: [xgb_os#P25] 1-th Feature selection (9.86s).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132311)\u001b[0m - # Orig. Feat.: 191 (# Cat. = 26; # Num. = 165)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132311)\u001b[0m - # Sel. Feat.: 162 (# Cat. = 19; # Num. = 143)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132311)\u001b[0m [24-04-24 13:27:20] In progress: [xgb_os#P25] Oversampling.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132299)\u001b[0m [24-04-24 13:27:26] Success: [xgb_os#P11] Training (33.01s).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132300)\u001b[0m [24-04-24 13:27:23] Success: [xgb_os#P26] 1-th Feature selection (7.55s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132300)\u001b[0m - # Orig. Feat.: 191 (# Cat. = 26; # Num. = 165)\n",
      "\u001b[2m\u001b[36m(_train pid=1132300)\u001b[0m - # Sel. Feat.: 157 (# Cat. = 17; # Num. = 140)\n",
      "\u001b[2m\u001b[36m(_train pid=1132300)\u001b[0m [24-04-24 13:27:23] In progress: [xgb_os#P26] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=1132302)\u001b[0m [24-04-24 13:27:26] Success: [xgb_os#P27] 1-th Feature selection (7.75s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132302)\u001b[0m - # Orig. Feat.: 191 (# Cat. = 26; # Num. = 165)\n",
      "\u001b[2m\u001b[36m(_train pid=1132302)\u001b[0m - # Sel. Feat.: 166 (# Cat. = 18; # Num. = 148)\n",
      "\u001b[2m\u001b[36m(_train pid=1132302)\u001b[0m [24-04-24 13:27:26] In progress: [xgb_os#P27] Oversampling.\n",
      "\u001b[2m\u001b[36m(_train pid=1132310)\u001b[0m [24-04-24 13:27:32] Success: [xgb_os#P14] Training (33.66s).\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_train pid=1132304)\u001b[0m [24-04-24 13:28:38] Success: [xgb_os#P24] Oversampling (82.75s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132304)\u001b[0m [24-04-24 13:28:38] In progress: [xgb_os#P24] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=1132308)\u001b[0m [24-04-24 13:27:35] Success: [xgb_os#P18] Training (25.92s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132303)\u001b[0m [24-04-24 13:28:45] Success: [xgb_os#P20] Oversampling (102.39s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132303)\u001b[0m [24-04-24 13:28:45] In progress: [xgb_os#P20] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=1132309)\u001b[0m [24-04-24 13:28:51] Success: [xgb_os#P22] Oversampling (103.90s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132309)\u001b[0m [24-04-24 13:28:51] In progress: [xgb_os#P22] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=1132304)\u001b[0m [24-04-24 13:28:54] Success: [xgb_os#P24] Training (15.89s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132300)\u001b[0m [24-04-24 13:28:57] Success: [xgb_os#P26] Oversampling (94.65s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132300)\u001b[0m [24-04-24 13:28:57] In progress: [xgb_os#P26] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=1132305)\u001b[0m [24-04-24 13:28:59] Success: [xgb_os#P23] Oversampling (101.29s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132305)\u001b[0m [24-04-24 13:28:59] In progress: [xgb_os#P23] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=1132303)\u001b[0m [24-04-24 13:29:01] Success: [xgb_os#P20] Training (16.25s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132309)\u001b[0m [24-04-24 13:29:07] Success: [xgb_os#P22] Training (15.84s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132302)\u001b[0m [24-04-24 13:29:02] Success: [xgb_os#P27] Oversampling (95.53s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132302)\u001b[0m [24-04-24 13:29:02] In progress: [xgb_os#P27] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=1132300)\u001b[0m [24-04-24 13:29:14] Success: [xgb_os#P26] Training (16.32s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132305)\u001b[0m [24-04-24 13:29:15] Success: [xgb_os#P23] Training (16.48s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132314)\u001b[0m [24-04-24 13:29:26] Success: [xgb_os#P21] Oversampling (144.06s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132314)\u001b[0m [24-04-24 13:29:26] In progress: [xgb_os#P21] Training.\n",
      "\u001b[2m\u001b[36m(_train pid=1132302)\u001b[0m [24-04-24 13:29:18] Success: [xgb_os#P27] Training (16.28s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132314)\u001b[0m [24-04-24 13:29:40] Success: [xgb_os#P21] Training (14.10s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132311)\u001b[0m [24-04-24 13:29:51] Success: [xgb_os#P25] Oversampling (150.82s).\n",
      "\u001b[2m\u001b[36m(_train pid=1132311)\u001b[0m [24-04-24 13:29:51] In progress: [xgb_os#P25] Training.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "from sklearn.base import clone\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from eli5.sklearn.permutation_importance import PermutationImportance\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "ESTIMATOR_DUMMY = DummyClassifier(strategy='prior')\n",
    "ESTIMATOR_RF = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "# ESTIMATOR_XGB = EvXGBClassifier(\n",
    "#     random_state=RANDOM_STATE, \n",
    "#     eval_metric='logloss', \n",
    "#     eval_size=0.2,\n",
    "#     early_stopping_rounds=10, \n",
    "#     objective='binary:logistic', \n",
    "#     verbosity=0,\n",
    "#     learning_rate=0.01,\n",
    "# )\n",
    "\n",
    "\n",
    "ESTIMATOR_XGB = EvXGBClassifier(\n",
    "    random_state=RANDOM_STATE, \n",
    "    eval_metric='auc', \n",
    "    eval_size=0.2, \n",
    "    early_stopping_rounds=20, \n",
    "    objective='binary:logistic', \n",
    "    verbosity=0,\n",
    "    learning_rate=0.01, \n",
    "    colsample_bytree = 0.8,\n",
    "    colsample_bylevel = 0.8,\n",
    "    scale_pos_weight =2,\n",
    "    min_child_weight =1,\n",
    "    subsample = 0.8,\n",
    "    max_depth =3,\n",
    "    gamma =0.1,\n",
    "    tree_method = 'hist',\n",
    "    n_estimators =1000,\n",
    "    reg_lambda = 1,\n",
    "    reg_alpha= 1,\n",
    "    num_parallel_tree =10\n",
    ")\n",
    "\n",
    "ESTIMATOR_LR = LogisticRegression(random_state = RANDOM_STATE, max_iter=500 )\n",
    "ESTIMATOR_KNN = KNeighborsClassifier()\n",
    "ESTIMATOR_SVM = SVC(probability=True)\n",
    "ESTIMATOR_GP = GaussianProcessClassifier(random_state=RANDOM_STATE)\n",
    "ESTIMATOR_DT = DecisionTreeClassifier(random_state = RANDOM_STATE)\n",
    "ESTIMATOR_MLP = MLPClassifier(random_state=RANDOM_STATE, max_iter=2000)\n",
    "ESTIMATOR_ADAB = AdaBoostClassifier(random_state=RANDOM_STATE)\n",
    "ESTIMATOR_GNB = GaussianNB()\n",
    "ESTIMATOR_QDA = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "\n",
    "SELECT_SVC = SelectFromModel(\n",
    "#     estimator=LinearSVC(\n",
    "#         penalty='l1',\n",
    "#         loss='squared_hinge',\n",
    "#         dual=False,\n",
    "#         tol=1e-3,\n",
    "#         C=1e-2,\n",
    "#         max_iter=5000,\n",
    "#         random_state=RANDOM_STATE\n",
    "#     ),\n",
    "#     threshold=1e-5\n",
    "    \n",
    "        estimator=LogisticRegression(\n",
    "        penalty='l1' \n",
    "        ,solver='liblinear'\n",
    "        , C=1, random_state=RANDOM_STATE, max_iter=4000\n",
    "    ),\n",
    "    threshold = 0.005\n",
    ")\n",
    "\n",
    "# CLS = ['valence', 'arousal', 'stress', 'disturbance']\n",
    "CLS = ['stress']\n",
    "\n",
    "SETTINGS = [\n",
    "    # dict(\n",
    "    #     estimator=clone(ESTIMATOR_DUMMY),\n",
    "    #     oversample=False,\n",
    "    #     select=None,\n",
    "    #     name='dummy'\n",
    "    # ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_RF),\n",
    "#         oversample=False,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='rf_ns'\n",
    "#     ),\n",
    "    # dict(\n",
    "    #     estimator=clone(ESTIMATOR_RF),\n",
    "    #     oversample=True,\n",
    "    #     select=[clone(SELECT_SVC)],\n",
    "    #     name='rf_os'\n",
    "    # ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_XGB),\n",
    "#         oversample=False,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='xgb_ns'\n",
    "#     ),\n",
    "    dict(\n",
    "        estimator=clone(ESTIMATOR_XGB),\n",
    "        oversample=True,\n",
    "        select=[clone(SELECT_SVC)],\n",
    "        name='xgb_os'\n",
    "    ),\n",
    "    # dict(\n",
    "    #     estimator=clone(ESTIMATOR_LR),\n",
    "    #     oversample=True,\n",
    "    #     select=[clone(SELECT_SVC)],\n",
    "    #     name='lr_os'\n",
    "    # ),\n",
    "    # dict(\n",
    "    #     estimator=clone(ESTIMATOR_KNN),\n",
    "    #     oversample=True,\n",
    "    #     select=[clone(SELECT_SVC)],\n",
    "    #     name='knn_os'\n",
    "    # ),\n",
    "    # dict(\n",
    "    #     estimator=clone(ESTIMATOR_SVM),\n",
    "    #     oversample=True,\n",
    "    #     select=[clone(SELECT_SVC)],\n",
    "    #     name='svm_os'\n",
    "    # ),\n",
    "    # dict(\n",
    "    #     estimator=clone(ESTIMATOR_DT),\n",
    "    #     oversample=True,\n",
    "    #     select=[clone(SELECT_SVC)],\n",
    "    #     name='dt_os'\n",
    "    # ),\n",
    "    # dict(\n",
    "    #     estimator=clone(ESTIMATOR_MLP),\n",
    "    #     oversample=True,\n",
    "    #     select=[clone(SELECT_SVC)],\n",
    "    #     name='mlp_os'\n",
    "    # ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_ADAB),merge\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='gp_os'\n",
    "#     ),\n",
    "    #         dict(\n",
    "    #     estimator=clone(ESTIMATOR_GNB),\n",
    "    #     oversample=True,\n",
    "    #     select=[clone(SELECT_SVC)],\n",
    "    #     name='gnb_os'\n",
    "    # ),\n",
    "#             dict(\n",
    "#         estimator=clone(ESTIMATOR_QDA),\n",
    "#         oversample=True,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='qda_os'\n",
    "#     ),\n",
    "#     dict(\n",
    "#         estimator=clone(ESTIMATOR_LSTM),\n",
    "#         oversample=None,\n",
    "#         lookback=lookback,\n",
    "#         select=[clone(SELECT_SVC)],\n",
    "#         name='lstm_os'\n",
    "#     )\n",
    "]\n",
    "\n",
    "\n",
    "#The following is dataset for normalization for each user\n",
    "# p = os.path.join(PATH_INTERMEDIATE, 'feat',f'stress-fixed-normalized.pkl')\n",
    "#The following is dataset without normalization for each user\n",
    "p = os.path.join(PATH_INTERMEDIATE, 'feat',f'stress-fixed-15min.pkl')\n",
    "\n",
    "par_dir = os.path.join(PATH_INTERMEDIATE, 'eval', 'stress')\n",
    "\n",
    "if os.path.isdir(par_dir):\n",
    "    # Get a list of all the files in the folder\n",
    "    files = os.listdir(par_dir)\n",
    "\n",
    "    # Delete all the files in the folder\n",
    "    for file in files:\n",
    "        if file !='.ipynb_checkpoints':\n",
    "            os.remove(os.path.join(par_dir, file))\n",
    "os.makedirs(par_dir, exist_ok=True)\n",
    "\n",
    "with on_ray():\n",
    "# with on_ray():\n",
    "    for l, s in product(\n",
    "        CLS, SETTINGS\n",
    "    ):       \n",
    "        X, y, groups, t, datetimes = load(p)\n",
    "        ##############################################\n",
    "        # #Remove users with extreme label distribution\n",
    "        # # Create a DataFrame from y, groups, t, datetimes\n",
    "        # info_df = pd.DataFrame({\n",
    "        #     'y': y,\n",
    "        #     'groups': groups,\n",
    "        #     't': t,\n",
    "        #     'datetimes': pd.to_datetime(datetimes)  # assuming 'datetimes' needs conversion to datetime\n",
    "        # })\n",
    "\n",
    "        # # Calculate majority/minority ratio for each group\n",
    "        # def calculate_ratio(group):\n",
    "        #     counts = group['y'].value_counts()\n",
    "        #     if len(counts) > 1:\n",
    "        #         majority = counts.max()\n",
    "        #         minority = counts.min()\n",
    "        #         ratio = majority / minority\n",
    "        #     else:\n",
    "        #         ratio = np.inf  # Infinite ratio if there's no minority class\n",
    "        #     return ratio\n",
    "\n",
    "        # # Apply the function per group\n",
    "        # group_ratios = info_df.groupby('groups').apply(calculate_ratio)\n",
    "\n",
    "        # # Filter groups based on the ratio\n",
    "        # filtered_groups = group_ratios[group_ratios <= 4].index\n",
    "\n",
    "        # # Filter the original DataFrame 'info_df' to remove skewed groups\n",
    "        # filtered_info = info_df[info_df['groups'].isin(filtered_groups)]\n",
    "\n",
    "        # # Use the indices of the filtered info to refine 'X'\n",
    "        # X_filtered = X.loc[filtered_info.index]\n",
    "\n",
    "        # # Extracting other arrays from the filtered info\n",
    "        # y_filtered = filtered_info['y'].values\n",
    "        # groups_filtered = filtered_info['groups'].values\n",
    "        # t_filtered = filtered_info['t'].values\n",
    "        # datetimes_filtered = filtered_info['datetimes'].values\n",
    "\n",
    "        # X, y, groups, t, datetimes = X_filtered, y_filtered, groups_filtered, t_filtered, datetimes_filtered\n",
    "\n",
    "        # Now 'X_filtered', 'y_filtered', 'groups_filtered', 't_filtered', 'datetimes_filtered'\n",
    "        # are ready to be used for further analysis or modeling\n",
    "        ################################################\n",
    "        # #Remove neutral state samples\n",
    "        # y =  LABELS_PROC['stressLevel'].to_numpy()\n",
    "\n",
    "        # # Create a mask that selects all samples where y is not equal to 3 (neutral state)\n",
    "        # mask = y != 3\n",
    "\n",
    "        # # Apply this mask to filter out the neutral samples from all arrays\n",
    "        # X_filtered = X[mask]  # X is a DataFrame, it uses boolean indexing directly\n",
    "        # y_filtered = y[mask]  # y, groups, t, datetimes are numpy arrays or similar structures\n",
    "        # groups_filtered = groups[mask]\n",
    "        # t_filtered = t[mask]\n",
    "        # datetimes_filtered = datetimes[mask]\n",
    "\n",
    "        # y = (y_filtered > 3).astype(int)\n",
    "        # X = X_filtered\n",
    "        # groups = groups_filtered\n",
    "        # t = t_filtered\n",
    "        # datetimes = datetimes_filtered\n",
    "\n",
    "        ################################################\n",
    "        #Use mean threshold for all users (only training set,\\ \n",
    "        #we need to use raw value and binarize after data splitting)\n",
    "        # y =  LABELS_PROC['stressLevel'].to_numpy()\n",
    "        #Use user speicifc mean threshold\n",
    "        # y =LABELS_PROC['stress_user_mean'].to_numpy()\n",
    "        #Use fixed threshold\n",
    "#         y =LABELS_PROC['stress_fixed'].to_numpy()\n",
    "        #Use three categories (fixed threshold) \n",
    "#        y =LABELS_PROC['stress_fixed_tri'].to_numpy()\n",
    "\n",
    "        \n",
    "        #The following code is designed for reordering for the sake of time series split \n",
    "        #################################################\n",
    "        # Create a DataFrame with user_id and datetime\n",
    "\n",
    "        df = pd.DataFrame({'user_id': groups, 'datetime': datetimes, 'label': y})\n",
    "\n",
    "        # df_merged = pd.merge(df, X, left_index=True, right_index=True)\n",
    "        df_merged = pd.merge(df, X, left_index=True, right_index=True)\n",
    "\n",
    "        # Normalize the datetime for each user and sort by datetime only needed for timeseries split/groupk partil personalization\n",
    "        #         df_merged['datetime'] = df_merged.groupby('user_id')['datetime'].transform(lambda x: x - x.min())\n",
    "        # df_merged['datetime'] = df_merged.groupby('user_id')['datetime'].transform(lambda x: x - x.min().normalize())\n",
    "\n",
    "        # Sort the DataFrame by datetime\n",
    "        df_merged = df_merged.sort_values(by=['user_id', 'datetime'])\n",
    "        # df_merged = df_merged.sort_values(by=['datetime'])\n",
    "\n",
    "        # Shuffle the DataFrame\n",
    "        # df_merged = df_merged.sample(frac=1, random_state=RANDOM_STATE)\n",
    "\n",
    "        # Update groups and datetimes\n",
    "        groups = df_merged['user_id'].to_numpy()\n",
    "        datetimes = df_merged['datetime'].to_numpy()  \n",
    "        y = df_merged['label'].to_numpy()\n",
    "        X = df_merged.drop(columns=['user_id', 'datetime', 'label'])\n",
    "\n",
    "        #The following code is for shuffling the temporal order for all users\n",
    "        ########################################################\n",
    "\n",
    "        # # Assuming 'groups', 'datetimes', 'y', and 'X' are already defined and loaded\n",
    "        # # Create a DataFrame with user_id, datetime, and label\n",
    "        # df = pd.DataFrame({\n",
    "        #     'user_id': groups,\n",
    "        #     'datetime': datetimes,\n",
    "        #     'label': y\n",
    "        # })\n",
    "\n",
    "        # # Merge the new DataFrame with the features DataFrame 'X'\n",
    "        # # Ensure 'X' is indexed the same way as 'groups', 'datetimes', and 'y'\n",
    "        # df_merged = pd.merge(df, X, left_index=True, right_index=True)\n",
    "\n",
    "        # # Shuffle the DataFrame\n",
    "        # # This disregards the temporal ordering completely and randomizes all entries\n",
    "        # df_merged = df_merged.sample(frac=1, random_state=42)  # Use a fixed seed for reproducibility\n",
    "\n",
    "        # # Extract the shuffled 'groups', 'datetimes', 'y', and 'X' from the shuffled DataFrame\n",
    "        # groups_shuffled = df_merged['user_id'].to_numpy()\n",
    "        # datetimes_shuffled = df_merged['datetime'].to_numpy()\n",
    "        # y_shuffled = df_merged['label'].to_numpy()\n",
    "        # X_shuffled = df_merged.drop(columns=['user_id', 'datetime', 'label'])\n",
    "\n",
    "        # # Optionally, you can convert 'X_shuffled' back to the correct type if it needs to be a DataFrame\n",
    "        # X_shuffled = pd.DataFrame(X_shuffled, columns=X.columns)\n",
    "\n",
    "        # X, y, groups, datetimes = X_shuffled, y_shuffled, groups_shuffled, datetimes_shuffled\n",
    "\n",
    "\n",
    "        #The following code is for only using 1st day\n",
    "        ###########################################\n",
    "        # filtered_df = pd.read_csv(os.path.join(PATH_INTERMEDIATE,'exclude_1st_day.csv'),index_col=0)\n",
    "        # # filtered_df = pd.read_csv(os.path.join(PATH_INTERMEDIATE,'exclude_1st_week.csv'),index_col=0)\n",
    "        # X_filtered = X[~X.index.isin(filtered_df.index)]\n",
    "        # y_series = pd.Series(y, index=X.index)\n",
    "        # y_filtered = y_series[~y_series.index.isin(filtered_df.index)]\n",
    "        # y_filtered = y_filtered.values\n",
    "        # groups_series = pd.Series(groups, index=X.index)\n",
    "        # groups_filtered = groups_series[~groups_series.index.isin(filtered_df.index)]\n",
    "        # groups_filtered = groups_filtered.values\n",
    "        # X,y, groups=X_filtered,y_filtered, groups_filtered\n",
    "        # #The following code is for excluding using 1st day\n",
    "        # ###########################################\n",
    "        # # filtered_df = pd.read_csv(os.path.join(PATH_INTERMEDIATE,'exclude_1st_week.csv'),index_col=0)\n",
    "        # filtered_df = pd.read_csv(os.path.join(PATH_INTERMEDIATE,'exclude_1st_day.csv'),index_col=0)\n",
    "        # X_filtered = X[X.index.isin(filtered_df.index)]\n",
    "        # y_series = pd.Series(y, index=X.index)\n",
    "        # y_filtered = y_series[y_series.index.isin(filtered_df.index)]\n",
    "        # y_filtered = y_filtered.values\n",
    "        # groups_series = pd.Series(groups, index=X.index)\n",
    "        # groups_filtered = groups_series[groups_series.index.isin(filtered_df.index)]\n",
    "        # groups_filtered = groups_filtered.values\n",
    "        # datetimes_series = pd.Series(datetimes, index=X.index)\n",
    "        # datetimes_filtered = datetimes_series[datetimes_series.index.isin(filtered_df.index)]\n",
    "        # datetimes_filtered = datetimes_filtered.values\n",
    "        # X,y, groups, datetimes=X_filtered,y_filtered, groups_filtered, datetimes_filtered\n",
    "        \n",
    "        \n",
    "        ###########################################\n",
    "        #The following code is for similar-user model\n",
    "        ###########################################\n",
    "#         similar_user = pd.read_csv(os.path.join(PATH_INTERMEDIATE,  'similar_user.csv'))\n",
    "#         cluster_label = similar_user['cluster'].value_counts().index[0] #N number clusters\n",
    "#         similar_users_in_cluster = similar_user[similar_user['cluster'] == cluster_label]['pcode']\n",
    "\n",
    "#         # Check if each value in 'groups' is in 'similar_users_in_cluster'\n",
    "#         mask = np.isin(groups, similar_users_in_cluster)\n",
    "\n",
    "#         # Filter 'groups' based on the mask\n",
    "#         filtered_groups = groups[mask]\n",
    "#         # Filter 'X' and 'y' based on the mask\n",
    "#         X_filtered = X[mask]\n",
    "#         y_filtered = y[mask]\n",
    "#         X,y, groups=X_filtered,y_filtered, filtered_groups\n",
    "        ###########################################\n",
    "        #Remove low frequency features\n",
    "#         mask = ['CAE#', 'MED#', 'ONF#', 'PWS#', 'RNG#','MSG#' ]\n",
    "#         X = X.loc[:, [all(m not in str(x) for m in mask) for x in X.columns]]\n",
    "\n",
    "        #Divide the features into different categories\n",
    "        feat_current = X.loc[:,[('#VAL' in str(x)) or ('ESM#LastLabel' in str(x)) for x in X.keys()]]  \n",
    "        feat_dsc = X.loc[:,[('#DSC' in str(x))  for x in X.keys()]]  \n",
    "        feat_yesterday = X.loc[:,[('Yesterday' in str(x))  for x in X.keys()]]  \n",
    "        feat_today = X.loc[:,[('Today' in str(x))  for x in X.keys()]]  \n",
    "        feat_sleep = X.loc[:,[('Sleep' in str(x))  for x in X.keys()]]  \n",
    "        feat_time = X.loc[:,[('Time' in str(x))  for x in X.keys()]]  \n",
    "        feat_pif = X.loc[:,[('PIF' in str(x))  for x in X.keys()]]  \n",
    "        feat_ImmediatePast = X.loc[:,[('ImmediatePast_15' in str(x))  for x in X.keys()]]\n",
    "        #Divide the time window features into sensor/past stress label\n",
    "        feat_current_sensor = X.loc[:,[('#VAL' in str(x))  for x in X.keys()]]  \n",
    "        feat_current_ESM = X.loc[:,[('ESM#LastLabel' in str(x)) for x in X.keys()]]  \n",
    "        feat_ImmediatePast_sensor = feat_ImmediatePast.loc[:,[('ESM' not in str(x)) for x in feat_ImmediatePast.keys()]]  \n",
    "        feat_ImmediatePast_ESM = feat_ImmediatePast.loc[:,[('ESM'  in str(x)) for x in feat_ImmediatePast.keys()]]  \n",
    "        feat_today_sensor = feat_today.loc[:,[('ESM' not in str(x))  for x in feat_today.keys()]]  \n",
    "        feat_today_ESM = feat_today.loc[:,[('ESM'  in str(x)) for x in feat_today.keys()]]  \n",
    "        feat_yesterday_sensor = feat_yesterday.loc[:,[('ESM' not in str(x)) for x in feat_yesterday.keys()]]  \n",
    "        feat_yesterday_ESM = feat_yesterday.loc[:,[('ESM'  in str(x)) for x in feat_yesterday.keys()]]\n",
    "\n",
    "\n",
    "\n",
    "        #Prepare the final feature set\n",
    "        feat_baseline = pd.concat([ feat_time,feat_dsc,feat_current_sensor, feat_ImmediatePast_sensor],axis=1)\n",
    "        #The following code is for calculating aggregated features\n",
    "        ########################################################################\n",
    "        # # Define a function to split the column name into sensor and attribute\n",
    "        # def split_column_name(col_name):\n",
    "        #     parts = col_name.rsplit(\"#\", 1)  # Split on last occurrence of '#'\n",
    "        #     return parts[0]  # This gives you 'Sensor#Attribute'\n",
    "\n",
    "        # # Get a list of unique sensor-attribute combinations\n",
    "        # df=feat_today_sensor\n",
    "        # sensor_attributes = df.columns.map(split_column_name).unique()\n",
    "\n",
    "        # # Create a list to hold the aggregated results\n",
    "        # agg_results = []\n",
    "\n",
    "        # # Loop over each sensor-attribute, select the appropriate columns, compute the mean and std\n",
    "        # for sensor_attribute in sensor_attributes:\n",
    "        #     # Select columns for this sensor-attribute\n",
    "        #     cols_to_aggregate = [col for col in df.columns if col.startswith(sensor_attribute)]\n",
    "        #     # Compute the mean and std and store in the new DataFrame\n",
    "        #     agg_results.append(df[cols_to_aggregate].mean(axis=1).rename(sensor_attribute + '|'+ 'MEAN'))\n",
    "        #     agg_results.append(df[cols_to_aggregate].std(axis=1).rename(sensor_attribute + '|'+'STD'))\n",
    "\n",
    "        # # Concatenate all the results into a single DataFrame\n",
    "        # agg_feature = pd.concat(agg_results, axis=1)\n",
    "\n",
    "        ######################################################################\n",
    "        feat_final = pd.concat([feat_baseline, feat_current_ESM],axis=1)\n",
    "        \n",
    "#         # Fill NaN values with zeros\n",
    "#         feat_final = feat_final.fillna(0)\n",
    "\n",
    "#         # Find the maximum non-infinity value and minimum non-negative infinity value across the entire dataframe\n",
    "#         max_val = feat_final[feat_final != np.inf].max().max()\n",
    "#         min_val = feat_final[feat_final != -np.inf].min().min()\n",
    "\n",
    "#         # Replace positive and negative infinity values\n",
    "#         feat_final.replace(np.inf, max_val, inplace=True)\n",
    "#         feat_final.replace(-np.inf, min_val, inplace=True)\n",
    "        \n",
    "        X = feat_final\n",
    "        \n",
    "        cats = X.columns[X.dtypes == bool]\n",
    "        \n",
    "        cross_val(\n",
    "            X=X, y=y, groups=groups, datetimes =datetimes,\n",
    "            path=par_dir,\n",
    "            normalize=True,\n",
    "            split='logo',\n",
    "            categories=cats,\n",
    "            split_params={'n_splits' : 5},\n",
    "            random_state=RANDOM_STATE,\n",
    "            **s\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sci-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
