{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Funcs.Utility import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Callable, Union, Tuple, List, Optional, Iterable\n",
    "from datetime import timedelta as td\n",
    "from scipy import stats\n",
    "import ray\n",
    "import warnings\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _safe_na_check(_v):\n",
    "    _is_nan_inf = False\n",
    "    \n",
    "    try:\n",
    "        _is_nan_inf = np.isnan(_v) or np.isinf(_v)\n",
    "    except:\n",
    "        _is_nan_inf = False\n",
    "    \n",
    "    return _is_nan_inf or _v is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_numeric_feature(d_key, d_val) -> Dict:\n",
    "    feature = {}\n",
    "    v=d_val\n",
    "    hist, _ = np.histogram(v, bins='doane', density=False)\n",
    "    std = np.sqrt(np.var(v, ddof=1)) if len(v) > 1 else 0\n",
    "    v_norm = (v - np.mean(v)) / std if std != 0 else np.zeros(len(v))\n",
    "    feature[f'{d_key}#AVG'] = np.mean(v) # Sample mean\n",
    "    feature[f'{d_key}#STD'] = std # Sample standard deviation\n",
    "    if std !=0:\n",
    "        feature[f'{d_key}#SKW'] = stats.skew(v, bias=False) # Sample skewness\n",
    "        feature[f'{d_key}#KUR'] = stats.kurtosis(v, bias=False) # Sample kurtosis\n",
    "    else:\n",
    "        feature[f'{d_key}#SKW'] = 0 # Sample skewness\n",
    "        feature[f'{d_key}#KUR'] = 0 # Sample c\n",
    "    feature[f'{d_key}#ASC'] = np.sum(np.abs(np.diff(v))) # Abstract sum of changes\n",
    "    feature[f'{d_key}#BEP'] = stats.entropy(hist) # Binned entropy\n",
    "    feature[f'{d_key}#MED'] = np.median(v) # Median\n",
    "    feature[f'{d_key}#TSC'] = np.sqrt(np.sum(np.power(np.diff(v_norm), 2))) # Timeseries complexity\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_categorical_feature(cats, d_key, d_val) -> Dict:\n",
    "    feature = {}\n",
    "    v = d_val\n",
    "    cnt = v.value_counts()\n",
    "    val, sup = cnt.index, cnt.values\n",
    "    hist = {k: v for k, v in zip(val, sup)}\n",
    "\n",
    "    # Information Entropy\n",
    "    feature[f'{d_key}#ETP#'] = stats.entropy(sup / len(v))\n",
    "    # Abs. Sum of Changes\n",
    "    feature[f'{d_key}#ASC#'] = np.sum(v.values[1:] != v.values[:-1])\n",
    "    if len(cats) == 2: # Dichotomous categorical data\n",
    "        c = cats[0]\n",
    "        feature[f'{d_key}#RLV_SUP'] = hist[c] / len(v) if c in hist else 0\n",
    "    else:\n",
    "        for c in cats:\n",
    "            feature[f'{d_key}#RLV_SUP={c}'] = hist[c] / len(v)  if c in hist else 0\n",
    "            \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_timeWindow_feature(is_numeric, cats, d_key, d_val) -> Dict:\n",
    "    feature = {}\n",
    "    v = d_val\n",
    "\n",
    "    if d_key in ['LOC_CLS']:\n",
    "        feature = _extract_categorical_feature(cats, d_key, v)\n",
    "        feature['LOC#NumOfPlcVist'] = len(set(v))\n",
    "    else:\n",
    "        if is_numeric:\n",
    "            feature = _extract_numeric_feature(d_key, v)\n",
    "        else:\n",
    "            feature =_extract_categorical_feature(cats, d_key, v)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_names = {\n",
    "    0: 'Dawn',\n",
    "    1: 'Morning',\n",
    "    2: 'Afternoon',\n",
    "    3: 'LateAfternoon',\n",
    "    4: 'Evening',\n",
    "    5: 'Night'\n",
    "}\n",
    "def _extract(\n",
    "        pid: str,\n",
    "        data: Dict[str, pd.Series],\n",
    "        label: pd.Series,\n",
    "        label_values: List[str],\n",
    "#        window_data: Dict[str, Union[int, Callable[[pd.Timestamp], int]]],\n",
    "#        window_label: Dict[str, Union[int, Callable[[pd.Timestamp], int]]],\n",
    "        categories: Dict[str, Optional[List[any]]] = None,\n",
    "        constant_features: Dict[str, any] = None,\n",
    "        resample_s: Dict[str, float] = None\n",
    ") -> Tuple[pd.DataFrame, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    _s = time.time()\n",
    "    log(f\"Begin feature extraction on {pid}'s data.\")\n",
    "    categories = categories or dict()\n",
    "    constant_features = constant_features or dict()\n",
    "    resample_s = resample_s or dict()\n",
    "    X, y, date_times = [], [], []\n",
    "#    count = 0\n",
    "    for timestamp in label.index:\n",
    "        row = dict()\n",
    "        #Find the start of today and yesterday for extracting today epoch features and yesterday epoch features\n",
    "        start_of_today = datetime.datetime(timestamp.year, timestamp.month, timestamp.day, tzinfo=timestamp.tzinfo)\n",
    "        start_of_today = pd.Timestamp(start_of_today.date(), tz=DEFAULT_TZ)\n",
    "        start_of_yesterday = timestamp - pd.Timedelta(days=1)\n",
    "        start_of_yesterday = pd.Timestamp(start_of_yesterday.date(), tz=DEFAULT_TZ)\n",
    "        label_cur = label.at[timestamp]\n",
    "        t = timestamp - td(milliseconds=1)\n",
    "\n",
    "        #Yesterday and Today 3-hour epochs\n",
    "        yesterday_time_windows_epoch = []\n",
    "        for i in range(6):\n",
    "            start = start_of_yesterday + pd.Timedelta(hours=i*3 + 6 )\n",
    "            end = start_of_yesterday + pd.Timedelta(hours=(i+1)*3 +6)\n",
    "            if start <= t:\n",
    "                yesterday_time_windows_epoch.append((start, min(end, t)))\n",
    "            else:\n",
    "                break\n",
    "        today_time_windows_epoch = []\n",
    "        for i in range(6):\n",
    "            start = start_of_today + pd.Timedelta(hours=i*3 +6)\n",
    "            end = start_of_today + pd.Timedelta(hours=(i+1)*3 + 6)\n",
    "            if start <= t:\n",
    "                today_time_windows_epoch.append((start, min(end, t)))\n",
    "            else:\n",
    "                break\n",
    "        #Yesterday and Today hourly windows\n",
    "#         yesterday_time_windows_hour = []\n",
    "#         for i in range(24):\n",
    "#             start = start_of_yesterday + pd.Timedelta(hours=i*1 )\n",
    "#             end = start_of_yesterday + pd.Timedelta(hours=(i+1)*1)\n",
    "#             if start <= t:\n",
    "#                 yesterday_time_windows_hour.append((start, min(end, t)))\n",
    "#             else:\n",
    "#                 break\n",
    "#         today_time_windows_hour = []\n",
    "#         for i in range(24):\n",
    "#             start = start_of_today + pd.Timedelta(hours=i*1 )\n",
    "#             end = start_of_today + pd.Timedelta(hours=(i+1)*1)\n",
    "#             if start <= t:\n",
    "#                 today_time_windows_hour.append((start, min(end, t)))\n",
    "#             else:\n",
    "#                 break\n",
    "        \n",
    "        # Features relevant to participants' info\n",
    "        for d_key, d_val in constant_features.items():\n",
    "            row[d_key] = d_val\n",
    "        # Features from sensor data\n",
    "        for d_key, d_val in data.items():\n",
    "            is_numeric = d_key not in categories\n",
    "            cats = categories.get(d_key) or list()\n",
    "            d_val = d_val.sort_index()\n",
    "            # Features relevant to latest value of a given data\n",
    "            # These features are extracted only for bounded categorical data and numerical data.\n",
    "            if is_numeric or cats:\n",
    "                try:\n",
    "                    v = d_val.loc[:t].iloc[-1]\n",
    "                except (KeyError, IndexError):\n",
    "                    v = 0\n",
    "                if is_numeric:\n",
    "                    row[f'{d_key}#VAL'] = v\n",
    "                else:\n",
    "                    for c in cats:\n",
    "                        row[f'{d_key}#VAL={c}'] = v == c\n",
    "            # Features relevant to duration since the latest state change.\n",
    "            # These features are only for categorical data.\n",
    "            # In addition, duration since a given state is set recently is considered,\n",
    "            # that are available only at bounded categorical data.\n",
    "            if not is_numeric:\n",
    "                try:\n",
    "                    v = d_val.loc[:t]\n",
    "                    row[f'{d_key}#DSC'] = (t - v.index[-1]).total_seconds() if len(v) else -1.0\n",
    "                    for c in cats:\n",
    "                        v_sub = v.loc[lambda x: x == c].index\n",
    "                        row[f'{d_key}#DSC={c}'] = (t - v_sub[-1]).total_seconds() if len(v_sub) else -1.0\n",
    "                except (KeyError, IndexError):\n",
    "                    row[f'{d_key}#DSC'] = 0\n",
    "                    for c in cats:\n",
    "                        row[f'{d_key}#DSC={c}'] = 0\n",
    "            # Features extracted from time-windows\n",
    "            # These features requires resampling and imputation on each data.\n",
    "            sample_rate = resample_s.get(d_key) or 1\n",
    "            d_val.drop_duplicates(inplace=True)\n",
    "            d_val_res = d_val.resample(f'{sample_rate}S', origin='start')\n",
    "            if is_numeric:\n",
    "                try:\n",
    "                    # Your resampling code here...\n",
    "                    d_val_res = d_val_res.mean().interpolate(method='linear').dropna()\n",
    "                except ValueError:\n",
    "                    # Save input data to a file or external storage for debugging...\n",
    "                    print(d_val_res)\n",
    "                    print(d_val)\n",
    "                    raise\n",
    "            else:\n",
    "                d_val_res = d_val_res.ffill().dropna()\n",
    "            #No resampling\n",
    "#             d_val_res =d_val\n",
    " \n",
    "           ###############################################################    \n",
    "           # Features extracted from 5-min immediate past time-windows\n",
    "            w_val = 5 * 60\n",
    "            try:\n",
    "                v = d_val_res.loc[t - td(seconds=w_val):t]\n",
    "            except (KeyError, IndexError):\n",
    "                continue\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                new_row = {f'{k}#ImmediatePast_5': v for k, v in _extract_timeWindow_feature(is_numeric, cats, d_key, v).items()}\n",
    "                row.update(new_row)\n",
    "            # Features extracted from 10-min immediate past time-windows\n",
    "            w_val = 10 * 60\n",
    "            try:\n",
    "                v = d_val_res.loc[t - td(seconds=w_val):t]\n",
    "            except (KeyError, IndexError):\n",
    "                continue\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                new_row = {f'{k}#ImmediatePast_10': v for k, v in _extract_timeWindow_feature(is_numeric, cats, d_key, v).items()}\n",
    "                row.update(new_row)\n",
    "\n",
    "           # Features extracted from 15-min immediate past time-windows\n",
    "            w_val = 15 * 60\n",
    "            try:\n",
    "                v = d_val_res.loc[t - td(seconds=w_val):t]\n",
    "            except (KeyError, IndexError):\n",
    "                continue\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                new_row = {f'{k}#ImmediatePast_15': v for k, v in _extract_timeWindow_feature(is_numeric, cats, d_key, v).items()}\n",
    "                row.update(new_row)\n",
    "\n",
    "            # Features extracted from 30-min immediate past time-windows\n",
    "            w_val = 30 * 60\n",
    "            try:\n",
    "                v = d_val_res.loc[t - td(seconds=w_val):t]\n",
    "            except (KeyError, IndexError):\n",
    "                continue\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                new_row = {f'{k}#ImmediatePast_30': v for k, v in _extract_timeWindow_feature(is_numeric, cats, d_key, v).items()}\n",
    "                row.update(new_row)\n",
    "\n",
    "            # Features extracted from 45-min immediate past time-windows\n",
    "            w_val = 45 * 60\n",
    "            try:\n",
    "                v = d_val_res.loc[t - td(seconds=w_val):t]\n",
    "            except (KeyError, IndexError):\n",
    "                continue\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                new_row = {f'{k}#ImmediatePast_45': v for k, v in _extract_timeWindow_feature(is_numeric, cats, d_key, v).items()}\n",
    "                row.update(new_row)\n",
    "                \n",
    "            #############################################################    \n",
    "            #Features extracted from yesterday epoch time windows\n",
    "            for count, (start, end) in enumerate(yesterday_time_windows_epoch):\n",
    "                # Get data for the current yesterday epoch time window\n",
    "                try:\n",
    "                    v = d_val_res.loc[start:end]\n",
    "                except (KeyError, IndexError):\n",
    "                    continue\n",
    "                epoch_name = epoch_names.get(count)\n",
    "\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter('ignore')\n",
    "                    new_row = {f'{k}#Yesterday{epoch_name}': v for k, v in _extract_timeWindow_feature(is_numeric, cats, d_key, v).items()}\n",
    "                    row.update(new_row)\n",
    "                    \n",
    "            #Features extracted from today epoch time windows until current time\n",
    "            for count, (start, end) in enumerate(today_time_windows_epoch):\n",
    "                # Get data for the current time window\n",
    "                try:\n",
    "                    v = d_val_res.loc[start:end]\n",
    "                except (KeyError, IndexError):\n",
    "                    continue\n",
    "                epoch_name = epoch_names.get(count)\n",
    "\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter('ignore')\n",
    "                    new_row = {f'{k}_Today{epoch_name}': v for k, v in _extract_timeWindow_feature(is_numeric, cats, d_key, v).items()}\n",
    "                    row.update(new_row)\n",
    "\n",
    "            \n",
    "        # Features relevant to time\n",
    "        day_of_week = ['MON', 'TUE', 'WED', 'THU', 'FRI', 'SAT', 'SUN'][t.isoweekday() - 1]\n",
    "        is_weekend = 'Y' if t.isoweekday() > 5 else 'N'\n",
    "        hour = t.hour\n",
    "\n",
    "        if 6 <= hour < 9:\n",
    "            hour_name = 'Dawn'\n",
    "        elif 9 <= hour < 12:\n",
    "            hour_name = 'MORNING'\n",
    "        elif 12 <= hour < 15:\n",
    "            hour_name = 'AFTERNOON'\n",
    "        elif 15 <= hour < 18:\n",
    "            hour_name = 'LATE_AFTERNOON'\n",
    "        elif 18 <= hour < 21:\n",
    "            hour_name = 'EVENING'\n",
    "        elif 21 <= hour < 24:\n",
    "            hour_name = 'NIGHT'\n",
    "        else:\n",
    "            hour_name = 'MIDNIGHT'\n",
    "            \n",
    "        for d in ['MON', 'TUE', 'WED', 'THU', 'FRI', 'SAT', 'SUN']:\n",
    "            row[f'Time#DOW={d}'] = d == day_of_week\n",
    "        for d in ['Y', 'N']:\n",
    "            row[f'Time#WKD={d}'] = d == is_weekend\n",
    "        for d in ['DAWN', 'MORNING', 'AFTERNOON', 'LATE_AFTERNOON', 'EVENING', 'NIGHT', 'MIDNIGHT']:\n",
    "            row[f'Time#HRN={d}'] = d == hour_name\n",
    "        \n",
    "\n",
    "        try:\n",
    "            last_label = label.loc[label[:t].index.max()]\n",
    "        except (KeyError, IndexError):\n",
    "            last_label = 0\n",
    "        row[f'ESM#LastLabel'] = last_label\n",
    "\n",
    "#############################################################################################\n",
    "        #The following code is designed for fixed threshold\n",
    "        # Label values extracted from yesterday epochs\n",
    "        for count, (start, end) in enumerate(yesterday_time_windows_epoch):\n",
    "            try:\n",
    "                v = label.loc[start:end]\n",
    "                epoch_name = epoch_names.get(count)\n",
    "                if len(label_values) <= 2: # Binary classification\n",
    "                    row[f'ESM#LIK#Yesterday{epoch_name}'] = np.sum(v == label_values[0]) / len(v) if len(v) > 0 else 0\n",
    "                else:\n",
    "                    for l in label_values:\n",
    "                        row[f'ESM#LIK#Yesterday{epoch_name}'] = np.sum(v == l) / len(v) if len(v) > 0 else 0\n",
    "            except (KeyError, IndexError):\n",
    "                epoch_name = epoch_names.get(count)\n",
    "                if len(label_values) <= 2:\n",
    "                    row[f'ESM#LIK#Yesterday{epoch_name}'] = 0\n",
    "                else:\n",
    "                    for l in label_values:\n",
    "                        row[f'ESM#LIK#Yesterday{epoch_name}'] = 0\n",
    "        # Label values extracted from today epochs\n",
    "        for count, (start, end) in enumerate(today_time_windows_epoch):\n",
    "            try:\n",
    "                v = label.loc[start:end]\n",
    "                epoch_name = epoch_names.get(count)\n",
    "                if len(label_values) <= 2: # Binary classification\n",
    "                    row[f'ESM#LIK#Today{epoch_name}'] = np.sum(v == label_values[0]) / len(v) if len(v) > 0 else 0\n",
    "                else:\n",
    "                    for l in label_values:\n",
    "                        row[f'ESM#LIK#Today{epoch_name}'] = np.sum(v == l) / len(v) if len(v) > 0 else 0\n",
    "            except (KeyError, IndexError):\n",
    "                epoch_name = epoch_names.get(count)\n",
    "                if len(label_values) <= 2:\n",
    "                    row[f'ESM#LIK#Today{epoch_name}'] = 0\n",
    "                else:\n",
    "                    for l in label_values:\n",
    "                        row[f'ESM#LIK#Today{epoch_name}'] = 0\n",
    "###############################################################################3\n",
    "          #The following code is designed for stress_dyn since we should not know previous binned label values\n",
    "\n",
    "        # Label values extracted from yesterday epochs\n",
    "#         for count, (start, end) in enumerate(yesterday_time_windows_epoch):\n",
    "#             try:\n",
    "#                 v = label.loc[start:end]\n",
    "#                 epoch_name = epoch_names.get(count)\n",
    "#                 row[f'ESM#Mean#Yesterday{epoch_name}'] = np.mean(v) if len(v) > 0 else 0\n",
    "#                 row[f'ESM#Std#Yesterday{epoch_name}'] = np.std(v) if len(v) > 0 else 0\n",
    "\n",
    "#             except (KeyError, IndexError):\n",
    "#                 epoch_name = epoch_names.get(count)\n",
    "#                 row[f'ESM#Mean#Yesterday{epoch_name}'] = 0 \n",
    "#                 row[f'ESM#Std#Yesterday{epoch_name}'] = 0\n",
    "                \n",
    "# #         # Label values extracted from today epochs\n",
    "#         for count, (start, end) in enumerate(today_time_windows_epoch):\n",
    "#             try:\n",
    "#                 v = label.loc[start:end]\n",
    "#                 epoch_name = epoch_names.get(count)\n",
    "#                 row[f'ESM#Mean#Today{epoch_name}'] = np.mean(v) if len(v) > 0 else 0\n",
    "#                 row[f'ESM#Std#Today{epoch_name}'] = np.std(v) if len(v) > 0 else 0\n",
    "\n",
    "#             except (KeyError, IndexError):\n",
    "#                 epoch_name = epoch_names.get(count)\n",
    "#                 row[f'ESM#Mean#Today{epoch_name}'] = 0  \n",
    "#                 row[f'ESM#Std#Today{epoch_name}'] = 0 \n",
    "############################################################################\n",
    "\n",
    "#         # Label values extracted from immediate past\n",
    "#         w_val = 15 * 60\n",
    "#         try:\n",
    "#             v = label.loc[t - td(seconds=w_val):t]\n",
    "#             epoch_name = epoch_names.get(count)\n",
    "#             if len(label_values) <= 2: # Binary classification\n",
    "#                 row[f'ESM#LIK#ImmediatePast'] = np.sum(v == label_values[0]) / len(v) if len(v) > 0 else 0\n",
    "#             else:\n",
    "#                 for l in label_values:\n",
    "#                     row[f'ESM#LIK={l}#ImmediatePast'] = np.sum(v == l) / len(v) if len(v) > 0 else 0\n",
    "#         except (KeyError, IndexError):\n",
    "#             epoch_name = epoch_names.get(count)\n",
    "#             if len(label_values) <= 2:\n",
    "#                 row[f'ESM#LIK#ImmediatePast'] = 0\n",
    "#             else:\n",
    "#                 for l in label_values:\n",
    "#                     row[f'ESM#LIK={l}#ImmediatePast'] = 0\n",
    "\n",
    "        row = {\n",
    "            k: 0 if _safe_na_check(v) else v\n",
    "            for k, v in row.items()\n",
    "        }\n",
    "\n",
    "        X.append(row)\n",
    "        y.append(label_cur)\n",
    "        date_times.append(timestamp)\n",
    "    \n",
    "    log(f\"Complete feature extraction on {pid}'s data ({time.time() - _s:.2f} s).\")\n",
    "    \n",
    "    #Without normalization for each user\n",
    "    X = pd.DataFrame(X)\n",
    "    y = np.asarray(y)\n",
    "    group = np.repeat(pid, len(y))\n",
    "    date_times =  np.asarray(date_times)\n",
    "\n",
    "    #Normalization for each feature of user pid\n",
    "    df_X = pd.DataFrame(X).fillna(0)\n",
    "    df_X_sensor =  df_X.loc[:,[('PIF' not in str(x)) and ('ESM' not in str(x)) for x in df_X.keys()]]  \n",
    "    df_X_no_sensor = df_X.loc[:,[('PIF' in str(x)) or ('ESM' in str(x)) for x in df_X.keys()]]\n",
    "    # Z-score Standardization for each column\n",
    "    X_normalized_sensor = df_X_sensor.apply(lambda col: (col - col.mean()) / col.std())\n",
    "    X_normalized = pd.concat([X_normalized_sensor, df_X_no_sensor], axis=1 )\n",
    "    X, y, group, date_times = X_normalized, np.asarray(y), np.repeat(pid, len(y)), np.asarray(date_times)\n",
    "    return X, y, group, date_times\n",
    "\n",
    "def extract(\n",
    "        pids: Iterable[str],\n",
    "        data: Dict[str, pd.Series],\n",
    "        label: pd.Series,\n",
    "        label_values: List[str],\n",
    "#        window_data: Dict[str, Union[int, Callable[[pd.Timestamp], int]]],\n",
    "#        window_label: Dict[str, Union[int, Callable[[pd.Timestamp], int]]],\n",
    "        categories: Dict[str, Optional[List[any]]] = None,\n",
    "        constat_features: Dict[str, Dict[str, any]] = None,\n",
    "        resample_s: Dict[str, float] = None,\n",
    "        with_ray: bool=False\n",
    "):\n",
    "    if with_ray and not ray.is_initialized():\n",
    "        raise EnvironmentError('Ray should be initialized if \"with_ray\" is set as True.')\n",
    "    func = ray.remote(_extract).remote if with_ray else _extract\n",
    "    jobs = []\n",
    "    for pid in pids:\n",
    "        d = dict()\n",
    "        for k, v in data.items():\n",
    "            try:\n",
    "                d[k] = v.loc[(pid, )]\n",
    "                if k.startswith('LOC_'):\n",
    "                    d[k].index= pd.to_datetime( d[k].index, unit='ms', utc=True).tz_convert(DEFAULT_TZ)\n",
    "                d['SPEED'] = d.pop('LOC_SPEED')\n",
    "            except (KeyError, IndexError):\n",
    "                pass\n",
    "        job = func(\n",
    "            pid=pid, data=d, label=label.loc[(pid, )],\n",
    "            label_values=label_values,\n",
    "#            window_data=window_data,\n",
    "#            window_label=window_label,\n",
    "            categories=categories,\n",
    "            constant_features=constat_features[pid],\n",
    "            resample_s=resample_s\n",
    "        )\n",
    "        jobs.append(job)\n",
    "    jobs = ray.get(jobs) if with_ray else jobs\n",
    "    print([x.shape for _, x, _, _ in jobs])\n",
    "    X = pd.concat([x for x, _, _, _ in jobs], axis=0, ignore_index=True)\n",
    "    y = np.concatenate([x for _, x, _, _ in jobs], axis=0)\n",
    "    group = np.concatenate([x for _, _, x, _ in jobs], axis=0)\n",
    "    date_times = np.concatenate([x for _, _, _, x in jobs], axis=0)\n",
    "    t_s = date_times.min().normalize().timestamp()\n",
    "    t_norm = np.asarray(list(map(lambda x: x.timestamp() - t_s, date_times)))\n",
    "    C, DTYPE = X.columns, X.dtypes\n",
    "    X = X.fillna({\n",
    "        **{c: False for c in C[(DTYPE == object) | (DTYPE == bool)]},\n",
    "        **{c: 0.0 for c in C[(DTYPE != object) & (DTYPE != bool)]},\n",
    "    }).astype({\n",
    "        **{c: 'bool' for c in C[(DTYPE == object) | (DTYPE == bool)]},\n",
    "        **{c: 'float32' for c in C[(DTYPE != object) & (DTYPE != bool)]},\n",
    "    })\n",
    "    return X, y, group, t_norm, date_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cloudpickle\n",
    "\n",
    "LABEL_VALUES = [1, 0]\n",
    "RESAMPLE_S = {\n",
    "    'ACC_AXX': 0.25,\n",
    "    'ACC_AXY': 0.25,\n",
    "    'ACC_AXZ': 0.25,\n",
    "    'ACC_MAG': 0.25,\n",
    "#     'AML': 1.0,\n",
    "    'EDA': 0.5,\n",
    "}\n",
    "\n",
    "\n",
    "CATEGORIES = {\n",
    "#    'DST_MOT': ['IDLE', 'WALKING', 'JOGGING', 'RUNNING'],\n",
    "#    'ULV_INT': ['NONE', 'LOW', 'MEDIUM', 'HIGH'],\n",
    "    'ACT': ['WALKING', 'STILL', 'IN_VEHICLE', 'ON_BICYCLE', 'RUNNING'],\n",
    "#    'APP_PAC': None,\n",
    "    'APP_CAT': ['SOCIAL','HEALTH','ENTER','WORK',\"INFO\"],\n",
    "#    'BAT_STA': ['CHARGING', 'DISCHARGING', 'FULL', 'NOT_CHARGING'],\n",
    "#    'CAE': ['CALL', 'IDLE'],\n",
    "#    'CON': ['DISCONNECTED', 'WIFI', 'MOBILE'],\n",
    "    'LOC_CLS': None,\n",
    "    'LOC_LABEL': ['eating','home','work','social','others'] ,\n",
    "    # 'SCR_EVENT':['ON', 'OFF', 'UNLOCK'],\n",
    "    # 'RNG': ['VIBRATE', 'SILENT', 'NORMAL'],\n",
    "    # 'CHG': ['DISCONNECTED', 'CONNECTED'],\n",
    "    # 'PWS': ['ACTIVATE', 'DEACTIVATE'],\n",
    "    # 'ONF': ['ON', 'OFF']\n",
    "}\n",
    "PARTICIPANTS = pd.read_csv(os.path.join(PATH_INTERMEDIATE, 'Preprocessed', 'PARTICIPANT_INFO.csv'),index_col = 'uid')\n",
    "PARTICIPANTS = PARTICIPANTS.rename(lambda x: 'P{:02}'.format(int(x)), axis='index')\n",
    "PINFO = PARTICIPANTS.assign(\n",
    "    AGE=lambda x: x['age'],\n",
    "    GEN=lambda x: x['gender'],\n",
    "    PSS=lambda x: x['PSS'],\n",
    ")[[\n",
    "    'AGE', 'GEN','PSS'\n",
    "]]\n",
    "\n",
    "PINFO = pd.get_dummies(PINFO, prefix_sep='=', dtype=bool).to_dict('index')\n",
    "PINFO = {k: {f'PIF#{x}': y for x, y in v.items()} for k, v in PINFO.items()}\n",
    "DATA = load(os.path.join(PATH_INTERMEDIATE, 'proc.pkl'))\n",
    "LABELS_PROC = pd.read_csv(os.path.join(PATH_INTERMEDIATE, 'Preprocessed', 'LABELS_PROC.csv'), index_col=['uid','timestamp'],parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 12:23:47,614\tINFO worker.py:1431 -- Connecting to existing Ray cluster at address: 143.248.57.77:6379...\n",
      "2024-04-22 12:23:47,640\tINFO worker.py:1612 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:376: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  d[k] = v.loc[(pid, )]\n",
      "/tmp/ipykernel_1070321/866121282.py:383: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  pid=pid, data=d, label=label.loc[(pid, )],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_extract pid=460537, ip=143.248.57.67)\u001b[0m [24-04-22 12:23:49] Begin feature extraction on P13's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=460530, ip=143.248.57.67)\u001b[0m [24-04-22 13:03:34] Complete feature extraction on P22's data (2385.31 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=1070415)\u001b[0m [24-04-22 12:23:51] Begin feature extraction on P19's data.\u001b[32m [repeated 23x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_extract pid=460533, ip=143.248.57.67)\u001b[0m [24-04-22 13:04:04] Complete feature extraction on P21's data (2415.42 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=460536, ip=143.248.57.67)\u001b[0m [24-04-22 13:04:59] Complete feature extraction on P03's data (2469.38 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=460537, ip=143.248.57.67)\u001b[0m [24-04-22 13:09:12] Complete feature extraction on P13's data (2723.61 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=460539, ip=143.248.57.67)\u001b[0m [24-04-22 13:17:01] Complete feature extraction on P08's data (3192.25 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=460535, ip=143.248.57.67)\u001b[0m [24-04-22 13:20:05] Complete feature extraction on P24's data (3375.27 s).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_extract pid=460532, ip=143.248.57.67)\u001b[0m [24-04-22 13:21:38] Complete feature extraction on P06's data (3468.97 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=1070409)\u001b[0m [24-04-22 14:07:37] Complete feature extraction on P14's data (6226.95 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=1070416)\u001b[0m [24-04-22 14:12:42] Complete feature extraction on P12's data (6532.31 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=1069757)\u001b[0m [24-04-22 14:42:19] Complete feature extraction on P27's data (8309.87 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=1069759)\u001b[0m [24-04-22 15:05:40] Complete feature extraction on P10's data (9710.98 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=1070424)\u001b[0m [24-04-22 15:08:30] Complete feature extraction on P11's data (9879.62 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=1069760)\u001b[0m [24-04-22 15:09:10] Complete feature extraction on P04's data (9921.04 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=1069762)\u001b[0m [24-04-22 15:17:45] Complete feature extraction on P20's data (10435.40 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=1069755)\u001b[0m [24-04-22 15:20:00] Complete feature extraction on P26's data (10569.97 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=1069756)\u001b[0m [24-04-22 15:21:37] Complete feature extraction on P23's data (10667.16 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=1070421)\u001b[0m [24-04-22 15:22:58] Complete feature extraction on P17's data (10748.54 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=1070412)\u001b[0m [24-04-22 15:23:44] Complete feature extraction on P02's data (10794.10 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=1069758)\u001b[0m [24-04-22 15:27:12] Complete feature extraction on P07's data (11002.93 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=1070417)\u001b[0m [24-04-22 15:27:23] Complete feature extraction on P05's data (11013.82 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=1070415)\u001b[0m [24-04-22 15:27:35] Complete feature extraction on P19's data (11024.13 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=1070418)\u001b[0m [24-04-22 15:42:09] Complete feature extraction on P25's data (11899.12 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=1069761)\u001b[0m [24-04-22 15:43:26] Complete feature extraction on P15's data (11976.92 s).\n",
      "[(590,), (774,), (447,), (520,), (491,), (643,), (435,), (527,), (268,), (296,), (524,), (805,), (564,), (619,), (595,), (452,), (513,), (431,), (638,), (473,), (690,), (645,), (726,), (468,)]\n"
     ]
    }
   ],
   "source": [
    "with on_ray():\n",
    "\n",
    "    feat = extract(\n",
    "        pids = LABELS_PROC.index.get_level_values('uid').unique(),\n",
    "        data = DATA,\n",
    "        categories=CATEGORIES,\n",
    "        label = LABELS_PROC['stress_fixed'],\n",
    "        resample_s=RESAMPLE_S, with_ray=True,\n",
    "        constat_features=PINFO,\n",
    "        label_values=LABEL_VALUES,\n",
    "    )\n",
    "    dump(feat, os.path.join(PATH_INTERMEDIATE, 'feat', f'stress-fixed-all-time-windows-normalized.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sci-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
