{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Funcs.Utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = os.path.join(PATH_INTERMEDIATE, 'feat',f'stress-fixed-15min.pkl')\n",
    "\n",
    "X, y, groups, t, datetimes = load(p)\n",
    "##############################################\n",
    "# #Remove users with extreme label distribution\n",
    "# # Create a DataFrame from y, groups, t, datetimes\n",
    "# info_df = pd.DataFrame({\n",
    "#     'y': y,\n",
    "#     'groups': groups,\n",
    "#     't': t,\n",
    "#     'datetimes': pd.to_datetime(datetimes)  # assuming 'datetimes' needs conversion to datetime\n",
    "# })\n",
    "\n",
    "# # Calculate majority/minority ratio for each group\n",
    "# def calculate_ratio(group):\n",
    "#     counts = group['y'].value_counts()\n",
    "#     if len(counts) > 1:\n",
    "#         majority = counts.max()\n",
    "#         minority = counts.min()\n",
    "#         ratio = majority / minority\n",
    "#     else:\n",
    "#         ratio = np.inf  # Infinite ratio if there's no minority class\n",
    "#     return ratio\n",
    "\n",
    "# # Apply the function per group\n",
    "# group_ratios = info_df.groupby('groups').apply(calculate_ratio)\n",
    "\n",
    "# # Filter groups based on the ratio\n",
    "# filtered_groups = group_ratios[group_ratios <= 4].index\n",
    "\n",
    "# # Filter the original DataFrame 'info_df' to remove skewed groups\n",
    "# filtered_info = info_df[info_df['groups'].isin(filtered_groups)]\n",
    "\n",
    "# # Use the indices of the filtered info to refine 'X'\n",
    "# X_filtered = X.loc[filtered_info.index]\n",
    "\n",
    "# # Extracting other arrays from the filtered info\n",
    "# y_filtered = filtered_info['y'].values\n",
    "# groups_filtered = filtered_info['groups'].values\n",
    "# t_filtered = filtered_info['t'].values\n",
    "# datetimes_filtered = filtered_info['datetimes'].values\n",
    "\n",
    "# X, y, groups, t, datetimes = X_filtered, y_filtered, groups_filtered, t_filtered, datetimes_filtered\n",
    "\n",
    "# Now 'X_filtered', 'y_filtered', 'groups_filtered', 't_filtered', 'datetimes_filtered'\n",
    "# are ready to be used for further analysis or modeling\n",
    "################################################\n",
    "# #Remove neutral state samples\n",
    "# y =  LABELS_PROC['stressLevel'].to_numpy()\n",
    "\n",
    "# # Create a mask that selects all samples where y is not equal to 3 (neutral state)\n",
    "# mask = y != 3\n",
    "\n",
    "# # Apply this mask to filter out the neutral samples from all arrays\n",
    "# X_filtered = X[mask]  # X is a DataFrame, it uses boolean indexing directly\n",
    "# y_filtered = y[mask]  # y, groups, t, datetimes are numpy arrays or similar structures\n",
    "# groups_filtered = groups[mask]\n",
    "# t_filtered = t[mask]\n",
    "# datetimes_filtered = datetimes[mask]\n",
    "\n",
    "# y = (y_filtered > 3).astype(int)\n",
    "# X = X_filtered\n",
    "# groups = groups_filtered\n",
    "# t = t_filtered\n",
    "# datetimes = datetimes_filtered\n",
    "\n",
    "################################################\n",
    "#Use mean threshold for all users (only training set,\\ \n",
    "#we need to use raw value and binarize after data splitting)\n",
    "# y =  LABELS_PROC['stressLevel'].to_numpy()\n",
    "#Use user speicifc mean threshold\n",
    "# y =LABELS_PROC['stress_user_mean'].to_numpy()\n",
    "#Use fixed threshold\n",
    "#         y =LABELS_PROC['stress_fixed'].to_numpy()\n",
    "#Use three categories (fixed threshold) \n",
    "#        y =LABELS_PROC['stress_fixed_tri'].to_numpy()\n",
    "\n",
    "\n",
    "#The following code is designed for reordering for the sake of time series split \n",
    "#################################################\n",
    "# Create a DataFrame with user_id and datetime\n",
    "\n",
    "df = pd.DataFrame({'user_id': groups, 'datetime': datetimes, 'label': y})\n",
    "\n",
    "# df_merged = pd.merge(df, X, left_index=True, right_index=True)\n",
    "df_merged = pd.merge(df, X, left_index=True, right_index=True)\n",
    "\n",
    "# Normalize the datetime for each user only needed for timeseries split/groupk partil personalization\n",
    "#         df_merged['datetime'] = df_merged.groupby('user_id')['datetime'].transform(lambda x: x - x.min())\n",
    "# df_merged['datetime'] = df_merged.groupby('user_id')['datetime'].transform(lambda x: x - x.min().normalize())\n",
    "\n",
    "# Sort the DataFrame by datetime\n",
    "df_merged = df_merged.sort_values(by=['user_id', 'datetime'])\n",
    "# df_merged = df_merged.sort_values(by=['datetime'])\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "# df_merged = df_merged.sample(frac=1, random_state=RANDOM_STATE)\n",
    "\n",
    "# Update groups and datetimes\n",
    "groups = df_merged['user_id'].to_numpy()\n",
    "datetimes = df_merged['datetime'].to_numpy()  \n",
    "y = df_merged['label'].to_numpy()\n",
    "X = df_merged.drop(columns=['user_id', 'datetime', 'label'])\n",
    "\n",
    "#The following code is for shuffling the temporal order for all users\n",
    "########################################################\n",
    "\n",
    "# # Assuming 'groups', 'datetimes', 'y', and 'X' are already defined and loaded\n",
    "# # Create a DataFrame with user_id, datetime, and label\n",
    "# df = pd.DataFrame({\n",
    "#     'user_id': groups,\n",
    "#     'datetime': datetimes,\n",
    "#     'label': y\n",
    "# })\n",
    "\n",
    "# # Merge the new DataFrame with the features DataFrame 'X'\n",
    "# # Ensure 'X' is indexed the same way as 'groups', 'datetimes', and 'y'\n",
    "# df_merged = pd.merge(df, X, left_index=True, right_index=True)\n",
    "\n",
    "# # Shuffle the DataFrame\n",
    "# # This disregards the temporal ordering completely and randomizes all entries\n",
    "# df_merged = df_merged.sample(frac=1, random_state=42)  # Use a fixed seed for reproducibility\n",
    "\n",
    "# # Extract the shuffled 'groups', 'datetimes', 'y', and 'X' from the shuffled DataFrame\n",
    "# groups_shuffled = df_merged['user_id'].to_numpy()\n",
    "# datetimes_shuffled = df_merged['datetime'].to_numpy()\n",
    "# y_shuffled = df_merged['label'].to_numpy()\n",
    "# X_shuffled = df_merged.drop(columns=['user_id', 'datetime', 'label'])\n",
    "\n",
    "# # Optionally, you can convert 'X_shuffled' back to the correct type if it needs to be a DataFrame\n",
    "# X_shuffled = pd.DataFrame(X_shuffled, columns=X.columns)\n",
    "\n",
    "# X, y, groups, datetimes = X_shuffled, y_shuffled, groups_shuffled, datetimes_shuffled\n",
    "\n",
    "\n",
    "#The following code is for only using 1st day\n",
    "###########################################\n",
    "# filtered_df = pd.read_csv(os.path.join(PATH_INTERMEDIATE,'exclude_1st_day.csv'),index_col=0)\n",
    "# # filtered_df = pd.read_csv(os.path.join(PATH_INTERMEDIATE,'exclude_1st_week.csv'),index_col=0)\n",
    "# X_filtered = X[~X.index.isin(filtered_df.index)]\n",
    "# y_series = pd.Series(y, index=X.index)\n",
    "# y_filtered = y_series[~y_series.index.isin(filtered_df.index)]\n",
    "# y_filtered = y_filtered.values\n",
    "# groups_series = pd.Series(groups, index=X.index)\n",
    "# groups_filtered = groups_series[~groups_series.index.isin(filtered_df.index)]\n",
    "# groups_filtered = groups_filtered.values\n",
    "# X,y, groups=X_filtered,y_filtered, groups_filtered\n",
    "# #The following code is for excluding using 1st day\n",
    "# ###########################################\n",
    "# # filtered_df = pd.read_csv(os.path.join(PATH_INTERMEDIATE,'exclude_1st_week.csv'),index_col=0)\n",
    "# filtered_df = pd.read_csv(os.path.join(PATH_INTERMEDIATE,'exclude_1st_day.csv'),index_col=0)\n",
    "# X_filtered = X[X.index.isin(filtered_df.index)]\n",
    "# y_series = pd.Series(y, index=X.index)\n",
    "# y_filtered = y_series[y_series.index.isin(filtered_df.index)]\n",
    "# y_filtered = y_filtered.values\n",
    "# groups_series = pd.Series(groups, index=X.index)\n",
    "# groups_filtered = groups_series[groups_series.index.isin(filtered_df.index)]\n",
    "# groups_filtered = groups_filtered.values\n",
    "# datetimes_series = pd.Series(datetimes, index=X.index)\n",
    "# datetimes_filtered = datetimes_series[datetimes_series.index.isin(filtered_df.index)]\n",
    "# datetimes_filtered = datetimes_filtered.values\n",
    "# X,y, groups, datetimes=X_filtered,y_filtered, groups_filtered, datetimes_filtered\n",
    "\n",
    "\n",
    "###########################################\n",
    "#The following code is for similar-user model\n",
    "###########################################\n",
    "#         similar_user = pd.read_csv(os.path.join(PATH_INTERMEDIATE,  'similar_user.csv'))\n",
    "#         cluster_label = similar_user['cluster'].value_counts().index[0] #N number clusters\n",
    "#         similar_users_in_cluster = similar_user[similar_user['cluster'] == cluster_label]['pcode']\n",
    "\n",
    "#         # Check if each value in 'groups' is in 'similar_users_in_cluster'\n",
    "#         mask = np.isin(groups, similar_users_in_cluster)\n",
    "\n",
    "#         # Filter 'groups' based on the mask\n",
    "#         filtered_groups = groups[mask]\n",
    "#         # Filter 'X' and 'y' based on the mask\n",
    "#         X_filtered = X[mask]\n",
    "#         y_filtered = y[mask]\n",
    "#         X,y, groups=X_filtered,y_filtered, filtered_groups\n",
    "###########################################\n",
    "#Remove low frequency features\n",
    "#         mask = ['CAE#', 'MED#', 'ONF#', 'PWS#', 'RNG#','MSG#' ]\n",
    "#         X = X.loc[:, [all(m not in str(x) for m in mask) for x in X.columns]]\n",
    "\n",
    "#Divide the features into different categories\n",
    "feat_current = X.loc[:,[('#VAL' in str(x)) or ('ESM#LastLabel' in str(x)) for x in X.keys()]]  \n",
    "feat_dsc = X.loc[:,[('#DSC' in str(x))  for x in X.keys()]]  \n",
    "feat_yesterday = X.loc[:,[('Yesterday' in str(x))  for x in X.keys()]]  \n",
    "feat_today = X.loc[:,[('Today' in str(x))  for x in X.keys()]]  \n",
    "feat_sleep = X.loc[:,[('Sleep' in str(x))  for x in X.keys()]]  \n",
    "feat_time = X.loc[:,[('Time' in str(x))  for x in X.keys()]]  \n",
    "feat_pif = X.loc[:,[('PIF' in str(x))  for x in X.keys()]]  \n",
    "feat_ImmediatePast = X.loc[:,[('ImmediatePast_15' in str(x))  for x in X.keys()]]\n",
    "#Divide the time window features into sensor/past stress label\n",
    "feat_current_sensor = X.loc[:,[('#VAL' in str(x))  for x in X.keys()]]  \n",
    "feat_current_ESM = X.loc[:,[('ESM#LastLabel' in str(x)) for x in X.keys()]]  \n",
    "feat_ImmediatePast_sensor = feat_ImmediatePast.loc[:,[('ESM' not in str(x)) for x in feat_ImmediatePast.keys()]]  \n",
    "feat_ImmediatePast_ESM = feat_ImmediatePast.loc[:,[('ESM'  in str(x)) for x in feat_ImmediatePast.keys()]]  \n",
    "feat_today_sensor = feat_today.loc[:,[('ESM' not in str(x))  for x in feat_today.keys()]]  \n",
    "feat_today_ESM = feat_today.loc[:,[('ESM'  in str(x)) for x in feat_today.keys()]]  \n",
    "feat_yesterday_sensor = feat_yesterday.loc[:,[('ESM' not in str(x)) for x in feat_yesterday.keys()]]  \n",
    "feat_yesterday_ESM = feat_yesterday.loc[:,[('ESM'  in str(x)) for x in feat_yesterday.keys()]]\n",
    "\n",
    "\n",
    "\n",
    "#Prepare the final feature set\n",
    "feat_baseline = pd.concat([ feat_time,feat_dsc,feat_current_sensor, feat_ImmediatePast_sensor],axis=1)\n",
    "#The following code is for calculating aggregated features\n",
    "########################################################################\n",
    "# # Define a function to split the column name into sensor and attribute\n",
    "# def split_column_name(col_name):\n",
    "#     parts = col_name.rsplit(\"#\", 1)  # Split on last occurrence of '#'\n",
    "#     return parts[0]  # This gives you 'Sensor#Attribute'\n",
    "\n",
    "# # Get a list of unique sensor-attribute combinations\n",
    "# df=feat_today_sensor\n",
    "# sensor_attributes = df.columns.map(split_column_name).unique()\n",
    "\n",
    "# # Create a list to hold the aggregated results\n",
    "# agg_results = []\n",
    "\n",
    "# # Loop over each sensor-attribute, select the appropriate columns, compute the mean and std\n",
    "# for sensor_attribute in sensor_attributes:\n",
    "#     # Select columns for this sensor-attribute\n",
    "#     cols_to_aggregate = [col for col in df.columns if col.startswith(sensor_attribute)]\n",
    "#     # Compute the mean and std and store in the new DataFrame\n",
    "#     agg_results.append(df[cols_to_aggregate].mean(axis=1).rename(sensor_attribute + '|'+ 'MEAN'))\n",
    "#     agg_results.append(df[cols_to_aggregate].std(axis=1).rename(sensor_attribute + '|'+'STD'))\n",
    "\n",
    "# # Concatenate all the results into a single DataFrame\n",
    "# agg_feature = pd.concat(agg_results, axis=1)\n",
    "\n",
    "######################################################################\n",
    "feat_final = pd.concat([feat_baseline],axis=1)\n",
    "\n",
    "#         # Fill NaN values with zeros\n",
    "#         feat_final = feat_final.fillna(0)\n",
    "\n",
    "#         # Find the maximum non-infinity value and minimum non-negative infinity value across the entire dataframe\n",
    "#         max_val = feat_final[feat_final != np.inf].max().max()\n",
    "#         min_val = feat_final[feat_final != -np.inf].min().min()\n",
    "\n",
    "#         # Replace positive and negative infinity values\n",
    "#         feat_final.replace(np.inf, max_val, inplace=True)\n",
    "#         feat_final.replace(-np.inf, min_val, inplace=True)\n",
    "\n",
    "X = feat_final\n",
    "cats = X.columns[X.dtypes == bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class EvXGBClassifier(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_size=None,\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=10,\n",
    "        random_state=None,\n",
    "        **kwargs\n",
    "        ):\n",
    "        self.random_state = random_state\n",
    "        self.eval_size = eval_size\n",
    "        self.eval_metric = eval_metric\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.model = XGBClassifier(\n",
    "            random_state=self.random_state,\n",
    "            eval_metric=self.eval_metric,\n",
    "            early_stopping_rounds=self.early_stopping_rounds,\n",
    "            # tree_method='gpu_hist',  # Use GPU histogram method\n",
    "            # predictor='gpu_predictor',  # Ensure predictions also use GPU\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def classes_(self):\n",
    "        return self.model.classes_\n",
    "\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        return self.model.feature_importances_\n",
    "    \n",
    "    @property\n",
    "    def feature_names_in_(self):\n",
    "        return self.model.feature_names_in_\n",
    "\n",
    "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: np.ndarray):\n",
    "        if self.eval_size:\n",
    "            splitter = StratifiedShuffleSplit(random_state=self.random_state, test_size=self.eval_size)\n",
    "            I_train, I_eval = next(splitter.split(X, y))\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X.iloc[I_eval, :], y[I_eval]\n",
    "            else:\n",
    "                X_train, y_train = X[I_train, :], y[I_train]\n",
    "                X_eval, y_eval = X[I_eval, :], y[I_eval]\n",
    "                \n",
    "            self.model = self.model.fit(\n",
    "                X=X_train, y=y_train, \n",
    "                eval_set=[(X_eval, y_eval)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            self.model = self.model.fit(X=X, y=y, verbose=False)\n",
    "        # After fitting, store the best iteration\n",
    "        self.best_iteration_ = self.model.get_booster().best_iteration\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "#         return self.model.predict(X)\n",
    "        return self.model.predict(X, iteration_range=(0, self.best_iteration_ + 1))\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame):\n",
    "#         return self.model.predict_proba(X)\n",
    "        return self.model.predict_proba(X, iteration_range=(0, self.best_iteration_ + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 21:04:27,914\tINFO worker.py:1612 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_fold pid=645922)\u001b[0m Training completed for Fold_22 with AUC: 0.5355537319823034\n",
      "\u001b[2m\u001b[36m(train_fold pid=645929)\u001b[0m Training completed for Fold_8 with AUC: 0.5804693921037412\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_fold pid=645914)\u001b[0m Training completed for Fold_14 with AUC: 0.47832\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_fold pid=645928)\u001b[0m Training completed for Fold_20 with AUC: 0.5144894483974943\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_fold pid=645925)\u001b[0m Training completed for Fold_23 with AUC: 0.44988807488807486\n",
      "\u001b[2m\u001b[36m(train_fold pid=645930)\u001b[0m Training completed for Fold_0 with AUC: 0.61597210692346\n",
      "\u001b[2m\u001b[36m(train_fold pid=645913)\u001b[0m Training completed for Fold_16 with AUC: 0.5222025939514728\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_fold pid=645911)\u001b[0m Training completed for Fold_15 with AUC: 0.5329736211031175\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_fold pid=645910)\u001b[0m Training completed for Fold_2 with AUC: 0.5286225402504472\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(train_fold pid=645908)\u001b[0m Training completed for Fold_11 with AUC: 0.5257388565891472\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "0.5221456449377896\n",
      "\u001b[2m\u001b[36m(train_fold pid=645918)\u001b[0m Training completed for Fold_17 with AUC: 0.5660980810234542\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import traceback\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import ray\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class FoldResult:\n",
    "    name: str\n",
    "    metrics: dict\n",
    "    duration: float\n",
    "\n",
    "def log(message: str):\n",
    "    print(message)  # Simple logging to stdout or enhance as needed\n",
    "\n",
    "@ray.remote\n",
    "def train_fold(dir_result: str, fold_name: str, X_train, y_train, X_test, y_test, C_cat, C_num, estimator, normalize, select, oversample, random_state):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        if normalize:\n",
    "            X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "            X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "            \n",
    "            scaler = StandardScaler().fit(X_train_N)\n",
    "            X_train_N = scaler.transform(X_train_N)\n",
    "            X_test_N = scaler.transform(X_test_N)\n",
    "        \n",
    "            X_train = pd.DataFrame(\n",
    "                np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "            X_test = pd.DataFrame(\n",
    "                np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "            \n",
    "        if select:\n",
    "            # # Removing low variance features\n",
    "            # X_train = exclude_low_variance(X_train)\n",
    "            # X_test = X_test[X_train.columns]  # Keep only the selected features in the test set\n",
    "\n",
    "            # #Removing highly correlated features\n",
    "            # X_train = remove_pairwise_corr(X_train, outcome_variable= y_train)\n",
    "            # X_test = X_test[X_train.columns]  # Keep only the selected features in the test set\n",
    "\n",
    "            if isinstance(select, SelectFromModel):\n",
    "                select = [select]\n",
    "                \n",
    "            for i, s in enumerate(select):\n",
    "                C = np.asarray(X_train.columns)\n",
    "                M = s.fit(X=X_train.values, y=y_train).get_support()\n",
    "                C_sel = C[M]\n",
    "                C_cat = C_cat[np.isin(C_cat, C_sel)]\n",
    "                C_num = C_num[np.isin(C_num, C_sel)]\n",
    "                \n",
    "                X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "                X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "\n",
    "\n",
    "                X_train = pd.DataFrame(\n",
    "                    np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                X_test = pd.DataFrame(\n",
    "                    np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "\n",
    "        if oversample:\n",
    "            if len(C_cat) > 0:\n",
    "                sampler = SMOTENC(categorical_features=[X_train.columns.get_loc(c) for c in C_cat], random_state=random_state)\n",
    "            else:\n",
    "                sampler = SMOTE(random_state=random_state)\n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        estimator = clone(estimator).fit(X_train, y_train)\n",
    "        y_pred = estimator.predict_proba(X_test)[:, 1]\n",
    "        auc_score = roc_auc_score(y_test, y_pred, average=None)\n",
    "\n",
    "        result = FoldResult(\n",
    "            name=fold_name,\n",
    "            metrics={'AUC': auc_score},\n",
    "            duration=time.time() - start_time\n",
    "        )\n",
    "        log(f'Training completed for {fold_name} with AUC: {auc_score}')\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f'Error in {fold_name}: {traceback.format_exc()}')\n",
    "        return None\n",
    "\n",
    "def perform_cross_validation(X, y, groups, estimator, normalize=False, select=None, oversample=False, random_state=None):\n",
    "    if not ray.is_initialized():\n",
    "        ray.init()\n",
    "\n",
    "    futures = []\n",
    "    splitter = LeaveOneGroupOut()  # Or any other CV strategy\n",
    "    for idx, (train_idx, test_idx) in enumerate(splitter.split(X, y, groups)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        C_cat = np.asarray(sorted(cats))\n",
    "        C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))\n",
    "\n",
    "        job = train_fold.remote('path_to_results', f'Fold_{idx}', X_train, y_train, X_test, y_test, C_cat, C_num, estimator, normalize, select, oversample, random_state)\n",
    "        futures.append(job)\n",
    "\n",
    "    results = ray.get(futures)\n",
    "    return results\n",
    "\n",
    "with on_ray():\n",
    "    SELECT_LASSO = SelectFromModel(\n",
    "            estimator=LogisticRegression(\n",
    "            penalty='l1' \n",
    "            ,solver='liblinear'\n",
    "            , C=1, random_state=RANDOM_STATE, max_iter=4000\n",
    "        ),\n",
    "        threshold = 0.005\n",
    "    )\n",
    "    # Example usage\n",
    "    estimator = EvXGBClassifier(\n",
    "        random_state=RANDOM_STATE, \n",
    "        eval_metric='logloss', \n",
    "        eval_size=0.2,\n",
    "        early_stopping_rounds=10, \n",
    "        objective='binary:logistic', \n",
    "        verbosity=0,\n",
    "        learning_rate=0.01,\n",
    "    )  \n",
    "    results = perform_cross_validation(X, y, groups, estimator, normalize=True, select=[SELECT_LASSO], oversample=True, random_state=42)\n",
    "    auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
    "    mean_auc = np.mean(auc_values)\n",
    "    print(mean_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-11-05 14:27:05</td></tr>\n",
       "<tr><td>Running for: </td><td>00:03:11.94        </td></tr>\n",
       "<tr><td>Memory:      </td><td>9.1/62.6 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using HyperBand: num_stopped=0 total_brackets=3<br>Round #0:<br>  Bracket(Max Size (n)=5, Milestone (r)=81, completed=0.0%): {RUNNING: 5} <br>  Bracket(Max Size (n)=8, Milestone (r)=27, completed=0.0%): {RUNNING: 8} <br>  Bracket(Max Size (n)=15, Milestone (r)=9, completed=0.0%): {PENDING: 1, RUNNING: 3} <br>Logical resource usage: 16.0/16 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name        </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  colsample_bylevel</th><th style=\"text-align: right;\">  colsample_bytree</th><th style=\"text-align: right;\">   early_stopping_round\n",
       "s</th><th style=\"text-align: right;\">    gamma</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  min_child_weight</th><th style=\"text-align: right;\">  n_estimators</th><th style=\"text-align: right;\">  num_parallel_tree</th><th style=\"text-align: right;\">  reg_alpha</th><th style=\"text-align: right;\">  reg_lambda</th><th style=\"text-align: right;\">  subsample</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_17ce0ad6</td><td>RUNNING </td><td>143.248.55.56:30820</td><td style=\"text-align: right;\">           0.595899</td><td style=\"text-align: right;\">          0.550121</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.0485247</td><td style=\"text-align: right;\">      0.0364722</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">                 6</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">   1.55554 </td><td style=\"text-align: right;\">    2.2488  </td><td style=\"text-align: right;\">   0.666677</td></tr>\n",
       "<tr><td>objective_fd2b246a</td><td>RUNNING </td><td>143.248.55.56:30847</td><td style=\"text-align: right;\">           0.700807</td><td style=\"text-align: right;\">          0.656362</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">0.142584 </td><td style=\"text-align: right;\">      0.0302242</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">   1.52867 </td><td style=\"text-align: right;\">    2.5236  </td><td style=\"text-align: right;\">   0.60177 </td></tr>\n",
       "<tr><td>objective_c0500300</td><td>RUNNING </td><td>143.248.55.56:30882</td><td style=\"text-align: right;\">           0.812503</td><td style=\"text-align: right;\">          0.870544</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">0.173309 </td><td style=\"text-align: right;\">      0.0858089</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                  1</td><td style=\"text-align: right;\">   1.91227 </td><td style=\"text-align: right;\">    2.53857 </td><td style=\"text-align: right;\">   0.61644 </td></tr>\n",
       "<tr><td>objective_58bab8e5</td><td>RUNNING </td><td>143.248.55.56:30914</td><td style=\"text-align: right;\">           0.953596</td><td style=\"text-align: right;\">          0.932819</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">0.0840241</td><td style=\"text-align: right;\">      0.0167395</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">   1.56675 </td><td style=\"text-align: right;\">    3.01407 </td><td style=\"text-align: right;\">   0.931538</td></tr>\n",
       "<tr><td>objective_560b5203</td><td>RUNNING </td><td>143.248.55.56:30997</td><td style=\"text-align: right;\">           0.802682</td><td style=\"text-align: right;\">          0.86318 </td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.0199912</td><td style=\"text-align: right;\">      0.185258 </td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">           250</td><td style=\"text-align: right;\">                  1</td><td style=\"text-align: right;\">   1.60485 </td><td style=\"text-align: right;\">    2.72798 </td><td style=\"text-align: right;\">   0.820453</td></tr>\n",
       "<tr><td>objective_9e5910d0</td><td>RUNNING </td><td>143.248.55.56:31024</td><td style=\"text-align: right;\">           0.780961</td><td style=\"text-align: right;\">          0.895238</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.416049 </td><td style=\"text-align: right;\">      0.0108013</td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                  1</td><td style=\"text-align: right;\">   1.56319 </td><td style=\"text-align: right;\">    1.23274 </td><td style=\"text-align: right;\">   0.696697</td></tr>\n",
       "<tr><td>objective_c3efd593</td><td>RUNNING </td><td>143.248.55.56:31051</td><td style=\"text-align: right;\">           0.609468</td><td style=\"text-align: right;\">          0.783928</td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.402549 </td><td style=\"text-align: right;\">      0.0424682</td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">   1.28708 </td><td style=\"text-align: right;\">    1.8315  </td><td style=\"text-align: right;\">   0.980248</td></tr>\n",
       "<tr><td>objective_5fc18720</td><td>RUNNING </td><td>143.248.55.56:31083</td><td style=\"text-align: right;\">           0.792334</td><td style=\"text-align: right;\">          0.560664</td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.156789 </td><td style=\"text-align: right;\">      0.150711 </td><td style=\"text-align: right;\">          6</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">   1.96763 </td><td style=\"text-align: right;\">    3.77582 </td><td style=\"text-align: right;\">   0.709496</td></tr>\n",
       "<tr><td>objective_a64f56a6</td><td>RUNNING </td><td>143.248.55.56:31110</td><td style=\"text-align: right;\">           0.807441</td><td style=\"text-align: right;\">          0.754508</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.0684057</td><td style=\"text-align: right;\">      0.0104192</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">                 6</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">                  1</td><td style=\"text-align: right;\">   1.77376 </td><td style=\"text-align: right;\">    0.694331</td><td style=\"text-align: right;\">   0.607356</td></tr>\n",
       "<tr><td>objective_2ddcb692</td><td>RUNNING </td><td>143.248.55.56:31137</td><td style=\"text-align: right;\">           0.570417</td><td style=\"text-align: right;\">          0.957471</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.486168 </td><td style=\"text-align: right;\">      0.0103536</td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">                  1</td><td style=\"text-align: right;\">   0.249157</td><td style=\"text-align: right;\">    3.00204 </td><td style=\"text-align: right;\">   0.66836 </td></tr>\n",
       "<tr><td>objective_37d7925d</td><td>RUNNING </td><td>143.248.55.56:31169</td><td style=\"text-align: right;\">           0.519218</td><td style=\"text-align: right;\">          0.853811</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.0487277</td><td style=\"text-align: right;\">      0.0141239</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">   1.70683 </td><td style=\"text-align: right;\">    0.544702</td><td style=\"text-align: right;\">   0.840213</td></tr>\n",
       "<tr><td>objective_e962d7f0</td><td>RUNNING </td><td>143.248.55.56:31197</td><td style=\"text-align: right;\">           0.632938</td><td style=\"text-align: right;\">          0.583826</td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.056565 </td><td style=\"text-align: right;\">      0.0263363</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                 5</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">   0.359796</td><td style=\"text-align: right;\">    4.24567 </td><td style=\"text-align: right;\">   0.951726</td></tr>\n",
       "<tr><td>objective_b2614c72</td><td>RUNNING </td><td>143.248.55.56:31230</td><td style=\"text-align: right;\">           0.700642</td><td style=\"text-align: right;\">          0.898841</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.13507  </td><td style=\"text-align: right;\">      0.108951 </td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">                 1</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">   0.166437</td><td style=\"text-align: right;\">    1.70833 </td><td style=\"text-align: right;\">   0.832669</td></tr>\n",
       "<tr><td>objective_f4a88b8d</td><td>RUNNING </td><td>143.248.55.56:31257</td><td style=\"text-align: right;\">           0.730374</td><td style=\"text-align: right;\">          0.561512</td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.433675 </td><td style=\"text-align: right;\">      0.175381 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">                 3</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">   0.470817</td><td style=\"text-align: right;\">    1.50927 </td><td style=\"text-align: right;\">   0.992596</td></tr>\n",
       "<tr><td>objective_10e3016d</td><td>RUNNING </td><td>143.248.55.56:31284</td><td style=\"text-align: right;\">           0.508844</td><td style=\"text-align: right;\">          0.53489 </td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.284656 </td><td style=\"text-align: right;\">      0.0672277</td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                 10</td><td style=\"text-align: right;\">   0.09242 </td><td style=\"text-align: right;\">    0.704651</td><td style=\"text-align: right;\">   0.679721</td></tr>\n",
       "<tr><td>objective_e8f27792</td><td>RUNNING </td><td>143.248.55.56:31316</td><td style=\"text-align: right;\">           0.606817</td><td style=\"text-align: right;\">          0.991751</td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">0.192804 </td><td style=\"text-align: right;\">      0.0426285</td><td style=\"text-align: right;\">          6</td><td style=\"text-align: right;\">                 2</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">   1.57114 </td><td style=\"text-align: right;\">    4.86086 </td><td style=\"text-align: right;\">   0.695086</td></tr>\n",
       "<tr><td>objective_cfe11d7e</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">           0.765361</td><td style=\"text-align: right;\">          0.553336</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">0.421648 </td><td style=\"text-align: right;\">      0.0137831</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">                 4</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">                 20</td><td style=\"text-align: right;\">   1.82297 </td><td style=\"text-align: right;\">    2.29295 </td><td style=\"text-align: right;\">   0.857885</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=30820)\u001b[0m Training completed for Fold_0 with AUC: 0.7509620698225532\n",
      "\u001b[2m\u001b[36m(objective pid=30914)\u001b[0m Training completed for Fold_0 with AUC: 0.7509620698225532\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=30882)\u001b[0m Training completed for Fold_0 with AUC: 0.7509620698225532\n",
      "\u001b[2m\u001b[36m(objective pid=31051)\u001b[0m Training completed for Fold_0 with AUC: 0.7509620698225532\n",
      "\u001b[2m\u001b[36m(objective pid=31083)\u001b[0m Training completed for Fold_0 with AUC: 0.7509620698225532\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=31137)\u001b[0m Training completed for Fold_0 with AUC: 0.7509620698225532\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=31197)\u001b[0m Training completed for Fold_0 with AUC: 0.7509620698225532\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=30914)\u001b[0m Training completed for Fold_1 with AUC: 0.7401357839824373\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=30882)\u001b[0m Training completed for Fold_1 with AUC: 0.7401357839824373\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=31051)\u001b[0m Training completed for Fold_1 with AUC: 0.7401357839824373\n",
      "\u001b[2m\u001b[36m(objective pid=31024)\u001b[0m Training completed for Fold_1 with AUC: 0.7401357839824373\n",
      "\u001b[2m\u001b[36m(objective pid=31110)\u001b[0m Training completed for Fold_1 with AUC: 0.7401357839824373\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=30820)\u001b[0m Training completed for Fold_2 with AUC: 0.7810923953956042\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=30914)\u001b[0m Training completed for Fold_2 with AUC: 0.7810923953956042\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=30882)\u001b[0m Training completed for Fold_2 with AUC: 0.7810923953956042\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=31024)\u001b[0m Training completed for Fold_2 with AUC: 0.7810923953956042\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=31083)\u001b[0m Training completed for Fold_2 with AUC: 0.7810923953956042\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=31197)\u001b[0m Training completed for Fold_2 with AUC: 0.7810923953956042\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=30820)\u001b[0m Training completed for Fold_3 with AUC: 0.7428849244841821\n",
      "\u001b[2m\u001b[36m(objective pid=31230)\u001b[0m Training completed for Fold_2 with AUC: 0.7810923953956042\n",
      "\u001b[2m\u001b[36m(objective pid=31316)\u001b[0m Training completed for Fold_2 with AUC: 0.7810923953956042\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=30882)\u001b[0m Training completed for Fold_3 with AUC: 0.7428849244841821\n",
      "\u001b[2m\u001b[36m(objective pid=31024)\u001b[0m Training completed for Fold_3 with AUC: 0.7428849244841821\n",
      "\u001b[2m\u001b[36m(objective pid=31110)\u001b[0m Training completed for Fold_3 with AUC: 0.7428849244841821\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=31197)\u001b[0m Training completed for Fold_3 with AUC: 0.7428849244841821\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=30847)\u001b[0m Training completed for Fold_4 with AUC: 0.7673825448117174\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=31284)\u001b[0m Training completed for Fold_3 with AUC: 0.7428849244841821\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(objective pid=31024)\u001b[0m Training completed for Fold_4 with AUC: 0.7673825448117174\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hyperopt import STATUS_OK, Trials, hp, fmin, tpe\n",
    "from sklearn.model_selection import LeaveOneGroupOut, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from ray import tune\n",
    "from ray.tune import with_parameters\n",
    "from ray.tune.schedulers import HyperBandScheduler\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "import traceback\n",
    "import ray\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class FoldResult:\n",
    "    name: str\n",
    "    metrics: dict\n",
    "    duration: float\n",
    "\n",
    "def log(message: str):\n",
    "    print(message)  # Simple logging to stdout or enhance as needed\n",
    "\n",
    "def train_fold(dir_result: str, fold_name: str, X_train, y_train, X_test, y_test, C_cat, C_num, estimator, normalize, select, oversample, random_state):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        if normalize:\n",
    "            X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "            X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "            \n",
    "            scaler = StandardScaler().fit(X_train_N)\n",
    "            X_train_N = scaler.transform(X_train_N)\n",
    "            X_test_N = scaler.transform(X_test_N)\n",
    "        \n",
    "            X_train = pd.DataFrame(\n",
    "                np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "            X_test = pd.DataFrame(\n",
    "                np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                columns=np.concatenate((C_cat, C_num))\n",
    "            )\n",
    "            \n",
    "        if select:\n",
    "            # # Removing low variance features\n",
    "            # X_train = exclude_low_variance(X_train)\n",
    "            # X_test = X_test[X_train.columns]  # Keep only the selected features in the test set\n",
    "\n",
    "            # #Removing highly correlated features\n",
    "            # X_train = remove_pairwise_corr(X_train, outcome_variable= y_train)\n",
    "            # X_test = X_test[X_train.columns]  # Keep only the selected features in the test set\n",
    "\n",
    "            if isinstance(select, SelectFromModel):\n",
    "                select = [select]\n",
    "                \n",
    "            for i, s in enumerate(select):\n",
    "                C = np.asarray(X_train.columns)\n",
    "                M = s.fit(X=X_train.values, y=y_train).get_support()\n",
    "                C_sel = C[M]\n",
    "                C_cat = C_cat[np.isin(C_cat, C_sel)]\n",
    "                C_num = C_num[np.isin(C_num, C_sel)]\n",
    "                \n",
    "                X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
    "                X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
    "\n",
    "                X_train = pd.DataFrame(\n",
    "                    np.concatenate((X_train_C, X_train_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "                X_test = pd.DataFrame(\n",
    "                    np.concatenate((X_test_C, X_test_N), axis=1),\n",
    "                    columns=np.concatenate((C_cat, C_num))\n",
    "                )\n",
    "\n",
    "        if oversample:\n",
    "            if len(C_cat) > 0:\n",
    "                sampler = SMOTENC(categorical_features=[X_train.columns.get_loc(c) for c in C_cat], random_state=random_state)\n",
    "            else:\n",
    "                sampler = SMOTE(random_state=random_state)\n",
    "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        estimator = clone(estimator).fit(X_train, y_train)\n",
    "        y_pred = estimator.predict_proba(X_test)[:, 1]\n",
    "        auc_score = roc_auc_score(y_test, y_pred, average=None)\n",
    "\n",
    "        result = FoldResult(\n",
    "            name=fold_name,\n",
    "            metrics={'AUC': auc_score},\n",
    "            duration=time.time() - start_time\n",
    "        )\n",
    "        log(f'Training completed for {fold_name} with AUC: {auc_score}')\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f'Error in {fold_name}: {traceback.format_exc()}')\n",
    "        return None\n",
    "\n",
    "def perform_cross_validation(X, y, groups, estimator, normalize=False, select=None, oversample=False, random_state=None):\n",
    "    if not ray.is_initialized():\n",
    "        ray.init()\n",
    "\n",
    "    futures = []\n",
    "    splitter = LeaveOneGroupOut()  # Or any other CV strategy\n",
    "    for idx, (train_idx, test_idx) in enumerate(splitter.split(X, y, groups)):\n",
    "        X_train_eval, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train_eval, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Split training set into internal training and evaluation sets\n",
    "        X_train, X_eval, y_train, y_eval = train_test_split(X_train_eval, y_train_eval, test_size=0.2, random_state=random_state)\n",
    "\n",
    "        C_cat = np.asarray(sorted(cats))\n",
    "        C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))\n",
    "\n",
    "        job = train_fold('path_to_results', f'Fold_{idx}', X_train, y_train, X_eval, y_eval, C_cat, C_num, estimator, normalize, select, oversample, random_state)\n",
    "        futures.append(job)\n",
    "\n",
    "    results = futures\n",
    "    return results\n",
    "\n",
    "def objective(params, X, y, groups):\n",
    "    SELECT_LASSO = SelectFromModel(\n",
    "            estimator=LogisticRegression(\n",
    "            penalty='l1' \n",
    "            ,solver='liblinear'\n",
    "            , C=1, random_state=RANDOM_STATE, max_iter=4000\n",
    "        ),\n",
    "        threshold = 0.005\n",
    "    )\n",
    "    # Example usage\n",
    "    estimator = EvXGBClassifier(\n",
    "        random_state=RANDOM_STATE, \n",
    "        eval_metric='logloss', \n",
    "        eval_size=0.2,\n",
    "        objective='binary:logistic', \n",
    "        verbosity=0,\n",
    "        **params\n",
    "    )  \n",
    "\n",
    "    results = perform_cross_validation(X, y, groups, estimator, normalize=True, select=[SELECT_LASSO], oversample=True, random_state=42)\n",
    "    auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
    "\n",
    "    mean_auc = np.mean(auc_values)\n",
    "    return {'loss': -mean_auc, 'auc': mean_auc, 'status': STATUS_OK}\n",
    "\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 11)),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.6, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1.0),\n",
    "    'gamma': hp.uniform('gamma', 0.0, 0.5),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 250, 500]),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.5, 5.0),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
    "    'num_parallel_tree': hp.choice('num_parallel_tree', [1, 10, 20]),\n",
    "    'early_stopping_rounds': hp.choice('early_stopping_rounds', [10, 30, 50]),\n",
    "}\n",
    "\n",
    "# Setup HyperOpt search with Ray Tune\n",
    "algo = HyperOptSearch(space, metric=\"auc\", mode=\"max\")\n",
    "\n",
    "# Define the scheduler for early stopping\n",
    "scheduler = HyperBandScheduler(time_attr=\"training_iteration\", metric=\"auc\", mode=\"max\")\n",
    "with on_ray():\n",
    "    # Assuming X, y, and groups are predefined datasets\n",
    "    analysis = tune.run(\n",
    "        with_parameters(objective, X=X, y=y, groups=groups),\n",
    "        num_samples=100,\n",
    "        search_alg=algo,\n",
    "        resources_per_trial={\"cpu\": 1},\n",
    "        verbose=1,\n",
    "        scheduler=scheduler\n",
    "        \n",
    "    )\n",
    "\n",
    "# Train the final model using the best hyperparameters\n",
    "best_params = analysis.best_config\n",
    "final_estimator = EvXGBClassifier(\n",
    "    random_state=RANDOM_STATE, \n",
    "    eval_metric='logloss', \n",
    "    eval_size=0.2,\n",
    "    objective='binary:logistic', \n",
    "    verbosity=0,\n",
    "    **best_params\n",
    ")\n",
    "final_estimator.fit(X, y)\n",
    "\n",
    "# Evaluate on the final test set\n",
    "y_pred = final_estimator.predict_proba(X)[:, 1]\n",
    "final_auc = roc_auc_score(y, y_pred)\n",
    "print(f'Final AUC on the held-out test set: {final_auc}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sci-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
